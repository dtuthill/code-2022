{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Softmax activation function\n",
    "\n",
    "When performing multiclass classification problems (like with the MNIST data set) we used the softmax activation function. In lecture I referred to this as the multiclass equivalent of the sigmoid function, let's see why now.\n",
    "\n",
    "Suppose we have some vector $z = (z_1, z_2, \\dots, z_K) \\in \\mathbb{R}^K$. The $i^\\text{th}$ entry of the softmax function applied to this vector is given by:\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "\n",
    "and so $\\sigma$ maps $\\mathbb{R}^K$ to $[0,1]^K$. As an example perhaps $z$ represents the values of your output nodes prior to activation, then the softmax turns these into \"probabilities\" of your observation being of class $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Backpropagation practice\n",
    "\n",
    "Look at this architecture that comes from the following blog post (don't cheat and just look up the solution though!), <a href=\"https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a>\n",
    "\n",
    "<img src=\"practice.png\" width=\"60%\"></img>\n",
    "\n",
    "This is both our first time with a multi-output network and a network with bias so I'll help you with the set up. To get started here are the formulas for $h_1$ and $h_2$.\n",
    "\n",
    "$$\n",
    "h_1 = \\Phi(w_1 x_1 + w_2 x_2 + b_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\Phi(w_3 x_1 + w_4 x_2 + b_1)\n",
    "$$\n",
    "\n",
    "Also in the case of a multi-output network our cost function is a sum of the two errors:\n",
    "$$\n",
    "C = (o_1 - y_1)^2 + (o_2 - y_2)^2,\n",
    "$$\n",
    "where $y = (y_1,y_2)$ and $o_1,o_2$ can be thought of as the $\\hat{y}$ in our simple example.\n",
    "\n",
    "For this problem:\n",
    "- Set up the equations for $o_1$ and $o_2$ in terms of $h_1$ and $h_2$,\n",
    "- Calculate $\\partial C/\\partial w_5$, $\\partial C/ \\partial w_1$ and $\\partial C/\\partial b_2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may record your answers in the markdown block if you would like to, math can be entered with typical latex commands with equations being contained in dollar signs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Exclusive or (XOR) I\n",
    "\n",
    "Recall that a failure of the perceptron was being able to produce nonlinear decision boundaries (or nonlinear regression in a regression setting). We demonstrated this with the following picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "y = np.array([1,-1,-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(X[y == 1,0],X[y == 1,1], c = 'b', label = \"1\", s=100)\n",
    "plt.scatter(X[y == -1,0],X[y == -1,1], c = 'orange', label = \"-1\", s=100)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\sigma = \\text{sgn}$, show that a perceptron with a bias term cannot separate these data points.\n",
    "\n",
    "<i>Hint</i>\n",
    "\n",
    "The setup for this perceptron would be:\n",
    "\n",
    "$$\n",
    "\\text{sgn}\\left( w_1 x_1 + w_2 x_2 + b \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Write here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Exclusive or II\n",
    "\n",
    "The classification problem above is roughly equivalent to building a classifier on these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((10000,2))\n",
    "\n",
    "X[:2500,:] = np.random.random((2500,2))\n",
    "X[2500:5000,:] = np.random.random((2500,2)) + np.array([2,2])\n",
    "X[5000:7500,:] = np.random.random((2500,2)) + np.array([0,2])\n",
    "X[7500:,:] = np.random.random((2500,2)) + np.array([2,0])\n",
    "\n",
    "y = np.zeros(10000)\n",
    "\n",
    "y[:2500] = 1\n",
    "y[2500:5000] = 1\n",
    "y[5000:7500] = -1\n",
    "y[7500:] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(X[y == 1,0],X[y == 1,1], c = 'b', label = \"1\", s=10)\n",
    "plt.scatter(X[y == -1,0],X[y == -1,1], c = 'orange', label = \"-1\", s=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sklearn` build a perceptron and a multilayer network with a single hidden layer of $100$ nodes, what is the accuracy of both on this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Additional `keras` metrics\n",
    "\n",
    "We used `\"accuracy\"` when specifying the metrics in the `compile` step. Look at the `keras` documentation on metrics to see what other metrics are available to us, <a href=\"https://keras.io/api/metrics/\">https://keras.io/api/metrics/</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. IMDB review sentiment\n",
    "\n",
    "In the code below I load in an IMDB review sentiment data set. In these data there are IMDB reviews and a corresponding sentiment $y=0$ indicating a negative review, $y=1$ indicating a positive review. In particular each observation in `X_train` or `X_test` will have $5000$ columns corresponding to the $5000$ most used words across all reviews. The $i,j$ entry of `X_train` or `X_test` will thus represent the frequency at which IMDB review $i$ utilized word $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the data\n",
    "from keras.datasets import imdb\n",
    "\n",
    "num_words = 5000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparing an X_train data set\n",
    "## We're making a series of word frequency vectors\n",
    "## Here column j is the frequency of word j in the review\n",
    "## this is a standard way to process text based data\n",
    "X_train_ff = np.zeros((len(train_data), num_words))\n",
    "X_test_ff = np.zeros((len(test_data), num_words))\n",
    "\n",
    "print(\"Starting word count vectorization for train data\")\n",
    "for i in range(len(train_data)):\n",
    "    for j in train_data[i]:\n",
    "        X_train_ff[i,j] = X_train_ff[i,j] + 1\n",
    "print(\"Done with word count vectorization for train data\")\n",
    "\n",
    "print(\"Starting word count vectorization for test data\")\n",
    "for i in range(len(test_data)):\n",
    "    for j in test_data[i]:\n",
    "        X_test_ff[i,j] = X_test_ff[i,j] + 1\n",
    "print(\"Done with word count vectorization for test data\")\n",
    "\n",
    "print(\"Now making word frequency vectors :)\")\n",
    "X_train_ff = X_train_ff/np.sum(X_train_ff, axis=1).reshape(-1,1)\n",
    "X_test_ff = X_test_ff/np.sum(X_test_ff, axis=1).reshape(-1,1)\n",
    "\n",
    "y_train_ff = train_labels.copy()\n",
    "y_test_ff = test_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The shape of X_train\n",
    "X_train_ff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The shape of X_test\n",
    "X_test_ff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrating the labeled data\n",
    "y_train_ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a validation set of $15\\%$ of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build two feed forward neural networks on these data:\n",
    "1. One with a single hidden layer with $64$ nodes,\n",
    "2. Another with two hidden layers each with $32$ nodes.\n",
    "\n",
    "Which one seems to perform better on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import what you'll need from keras here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Make an empty model object here\n",
    "model1 = \n",
    "\n",
    "## Add your layers here\n",
    "## the output layer should have a 'sigmoid' activation function\n",
    "\n",
    "\n",
    "\n",
    "## compile the model here\n",
    "\n",
    "\n",
    "\n",
    "## Fit your model here, don't forget to include your validation \n",
    "## data argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Make an empty model object here\n",
    "model2 = \n",
    "\n",
    "## Add your layers here\n",
    "\n",
    "\n",
    "\n",
    "## compile the model here\n",
    "\n",
    "\n",
    "\n",
    "## Fit your model here, don't forget to include your validation \n",
    "## data argument\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Adding Dropout Layers\n",
    "\n",
    "Sometimes you can combat overfitting by randomly dropping some of the nodes in the model. In `keras` this is accomplished with the `Dropout` layer. This layer will randomly set some of the nodes from the previous layer to $0$ with probability `rate` and scale the remaining nodes by $1/(1-\\text{rate})$.\n",
    "\n",
    "Return to the network you trained in the previous problem and add a `Dropout` layer using the code I provide below. Does this improve your model? (I'm not sure if it will in this particular model or not, but I wanted to give you a chance to practice adding a `Dropout` layer.\n",
    "\n",
    "Note you'll be plotting the validation and training accuracies of both models below, so store your history in a different variable than you used for question 2.\n",
    "\n",
    "Docs: <a href=\"https://keras.io/api/layers/regularization_layers/dropout/\">https://keras.io/api/layers/regularization_layers/dropout/</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Add your model from above.\n",
    "## Make an empty model object here\n",
    "model3 = \n",
    "\n",
    "## Add your layers here\n",
    "\n",
    "\n",
    "## prior to the output layer we'll add this dropout layer\n",
    "\n",
    "\n",
    "\n",
    "## compile the model here\n",
    "\n",
    "\n",
    "\n",
    "## Fit your model here, don't forget to include your validation \n",
    "## data argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Compare to `SimpleRNN`\n",
    "\n",
    "Run the code below to compare the feed forward results to the `SimpleRNN` network we built in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "(X_train, y_train), (X_test,y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "max_length = 100\n",
    "X_train_seq = sequence.pad_sequences(X_train, maxlen=100)\n",
    "X_test_seq = sequence.pad_sequences(X_test, maxlen=100)\n",
    "\n",
    "\n",
    "X_tt_seq,X_val_seq,y_tt_seq,y_val_seq = train_test_split(X_train_seq, y_train,\n",
    "                                                           test_size=.2,\n",
    "                                                           shuffle=True,\n",
    "                                                           stratify = y_train,\n",
    "                                                           random_state=440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del simple_rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn_model = models.Sequential()\n",
    "\n",
    "simple_rnn_model.add( layers.Embedding(max_features, 32) )\n",
    "simple_rnn_model.add( layers.SimpleRNN(32, return_sequences = False) )\n",
    "\n",
    "\n",
    "simple_rnn_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "simple_rnn_model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "history_snn = simple_rnn_model.fit(X_tt_seq, y_tt_seq,\n",
    "                                    epochs = epochs,\n",
    "                                    batch_size=500,\n",
    "                                    validation_data=(X_val_seq,y_val_seq))\n",
    "\n",
    "history_snn_dict = history_snn.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the validation accuracy of both models here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. `LSTM` layer\n",
    "\n",
    "An adjustment to the `SimpleRNN` architecture came in the form of the so-called \"Long Short-Term Memory\" or LSTM architecture. \n",
    "\n",
    "This adjustment addressed the issue of disappearing gradients that occurr due to the long sequences involved in the standard RNN architecture. These disappearing gradients during backpropagation mean that the networks pay much more attention to more recent terms in the sequence than further back terms.\n",
    "\n",
    "While I will not dive into the mathematical setup of LSTM networks here, check out section 7.5 of this text, <a href=\"https://d1wqtxts1xzle7.cloudfront.net/63954267/2018_Book_NeuralNetworksAndDeepLearning20200718-22595-1luren6-with-cover-page-v2.pdf?Expires=1652983581&Signature=LT2OEq4kN4bAjeVMo0Gi1B-JPuy0TUYR1VuGhVOnEiHc-bvoUY1-OHLSiLh8EAVQhMHG5U2x6Umg1muZArOvflSiZpDpTnKVMsjGdZYQs4CULVXGw~Zf4kl7jQiZJG4jRZZuA6m2-vxb9kykkEUqNLjdGATea2UJd9AbkkFUUnLUTWdLSNy5wSLKTKU~pYxYIrfhUZgUw4~pc9RBut4Z5L5W7bYhYhMyI10TTwqvtrzqMekCVLsZt8aNjqYkcYi1bBtsGT5yxqV85s6lfPezZaBR5xBvcccaga7zq9OKuwWwltiMhuldPUXZFt9jBGs5mu-kZsauNU0fvTCdKPA-QA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA\">https://d1wqtxts1xzle7.cloudfront.net/63954267/2018_Book_NeuralNetworksAndDeepLearning20200718-22595-1luren6-with-cover-page-v2.pdf?Expires=1652983581&Signature=LT2OEq4kN4bAjeVMo0Gi1B-JPuy0TUYR1VuGhVOnEiHc-bvoUY1-OHLSiLh8EAVQhMHG5U2x6Umg1muZArOvflSiZpDpTnKVMsjGdZYQs4CULVXGw~Zf4kl7jQiZJG4jRZZuA6m2-vxb9kykkEUqNLjdGATea2UJd9AbkkFUUnLUTWdLSNy5wSLKTKU~pYxYIrfhUZgUw4~pc9RBut4Z5L5W7bYhYhMyI10TTwqvtrzqMekCVLsZt8aNjqYkcYi1bBtsGT5yxqV85s6lfPezZaBR5xBvcccaga7zq9OKuwWwltiMhuldPUXZFt9jBGs5mu-kZsauNU0fvTCdKPA-QA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA</a>.\n",
    "\n",
    "In this problem you will walk through building an LSTM network on the IMDB data and comparing the performance to the previous two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Make an empty sequential model here\n",
    "lstm_model = \n",
    "\n",
    "## add the same embedding layer here\n",
    "lstm_model.add(  )\n",
    "\n",
    "## add the first LSTM layer\n",
    "## setting return_sequences=True allows us to add a second LSTM layer\n",
    "lstm_model.add( layers.LSTM(32, return_sequences = True) )\n",
    "\n",
    "## add the second LSTM layer\n",
    "## set return_sequences=False\n",
    "lstm_model.add( )\n",
    "\n",
    "\n",
    "## add the output layer\n",
    "\n",
    "\n",
    "## compile the model\n",
    "\n",
    "\n",
    "\n",
    "## train the model\n",
    "## Note this will take several minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the validation accuracy of all 3 models here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment on what you observe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common RNN architecture people use is a <i>gated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
