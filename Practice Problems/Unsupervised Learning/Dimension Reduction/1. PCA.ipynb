{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f063c4",
   "metadata": {},
   "source": [
    "# Principal Components Analysis (PCA)\n",
    "\n",
    "The problems in this notebook touch on principal components analysis. In particular this material corresponds to `Lectures/Unsupervised Learning/Dimension Reduction/1. Principal Components Analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44834443",
   "metadata": {},
   "source": [
    "##### 1. Lines of best fit\n",
    "\n",
    "Karl Pearson's 1903 paper, <a href = \"https://www.tandfonline.com/doi/abs/10.1080/14786440109462720\"><i>On lines and planes of closest fit to systems of points in space</i></a>, was the original introduction of PCA.\n",
    "\n",
    "In this paper Pearson contemplated what we mean by line (or plane) of best fit. In regression the approach is to find the line of best fit by minimizing the distance between the estimate, $\\hat{y}$, and the actual value, $y$. Pearson wondered what if instead we considered minimizing the distance between the data points and the hyperplane fit to those points.\n",
    "\n",
    "In the case of simple linear regression this translates to minimizing the collective length of the red lines pictured below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d565c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2*np.random.random(50) - 1\n",
    "y = x + .25*np.random.randn(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "slr = LinearRegression()\n",
    "slr.fit(x.reshape(-1,1), y)\n",
    "\n",
    "plt.figure(figsize=(7,9))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(np.linspace(-1,1,100), \n",
    "         slr.predict(np.linspace(-1,1,100).reshape(-1,1)), \n",
    "         'k',\n",
    "         zorder=1)\n",
    "\n",
    "for i in range(50):\n",
    "    plt.plot([x[i],x[i]],\n",
    "             [y[i],slr.predict([[x[i]]])[0]],\n",
    "             'r-',\n",
    "             zorder=1)\n",
    "    \n",
    "plt.scatter(x,y,s=100,zorder=50)\n",
    "\n",
    "plt.title('Linear Regression \"Line of Best Fit\"', fontsize=18)\n",
    "plt.xlabel(\"$x$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae6af6",
   "metadata": {},
   "source": [
    "For PCA on 2 features Pearson proposed minimizing the length of the red lines in this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "X = np.zeros((50,2))\n",
    "X[:,0] = x\n",
    "X[:,1] = y\n",
    "pca.fit(X)\n",
    "\n",
    "plt.figure(figsize=(7,9))\n",
    "\n",
    "plt.plot([-1.6*pca.components_[0][0], 1.6*pca.components_[0][0]],\n",
    "            [-1.6*pca.components_[0][1], 1.6*pca.components_[0][1]],\n",
    "            'k')\n",
    "\n",
    "for i in range(50):\n",
    "    plt.plot([x[i],X[i,:].dot(pca.components_[0])*pca.components_[0][0]],\n",
    "             [y[i],X[i,:].dot(pca.components_[0])*pca.components_[0][1]],\n",
    "             'r-',\n",
    "             zorder=1)\n",
    "    \n",
    "plt.scatter(x,y,s=100,zorder=50)\n",
    "\n",
    "plt.title('PCA \"Line of Best Fit\"', fontsize=18)\n",
    "plt.xlabel(\"$x$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c99e7",
   "metadata": {},
   "source": [
    "A similar image can be found in Pearson's paper,\n",
    "\n",
    "<img src = \"PearsonLine.png\" width = \"500\"></img>\n",
    "\n",
    "It turns out that this formulation is equivalent to maximizing the variance.\n",
    "\n",
    "Read through page 352 here, <a href=\"https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\">https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf</a> for a derivation of that fact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f174e8",
   "metadata": {},
   "source": [
    "##### While reading write any notes you would like here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05358d9",
   "metadata": {},
   "source": [
    "##### 2. The Relationship of PCA to SVD\n",
    "\n",
    "Recall that the PCA component vectors are given by the eigenvectors of $\\Sigma$, the covariance matrix of the data matrix $X$, where the columns of $X$ have all been centered at $0$.\n",
    "\n",
    "Any covariance matrix is symmetric, and thus is diagonalizable. So $\\Sigma = Q \\Lambda Q^T$, for some matrix $Q$, where $\\Lambda$ is a diagonal matrix of eigenvalues and $Q$ is a matrix of eigenvectors.\n",
    "\n",
    "In a potential abuse of notation let $X = U \\sigma V^T$ be the singular value decomposition of $X$.\n",
    "\n",
    "Now recall that the sample covariance matrix can be computed as $X^T X$.\n",
    "\n",
    "Recalculate $X^T X$ substituting in the SVD of $X$, what do you find? Hint: $V$ is orthonormal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55b413",
   "metadata": {},
   "source": [
    "##### Write here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d3f92",
   "metadata": {},
   "source": [
    "It is actually much safer to calculate the SVD of $X$ rather than find the eigenvectors of $X^T X$, and this is what `sklearn` does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0f533",
   "metadata": {},
   "source": [
    "##### 3. Kernel PCA\n",
    "\n",
    "PCA can fail to be useful to supervised learning problems when it is applied to nonlinear data, meaning data where the regression relationship is nonlinear or the decision boundary is nonlinear.\n",
    "\n",
    "When learning about support vector machines we learned about the \"kernel trick\", whose underlying principle is that we can lift nonlinear data into a higher dimensionsal space in which the data becomes linear. This same idea can be applied to PCA. <i>Kernel PCA</i> involves lifting the data to a higher dimensional space and then applying regular PCA on the lifted data. Luckily this can again be accomplished with the kernel trick.\n",
    "\n",
    "Kernel PCA can be implemented in `sklearn` with `KernelPCA`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998e4e2",
   "metadata": {},
   "source": [
    "##### 4. PCA Handles Multicolinearity\n",
    "\n",
    "Here's a very simple example of how PCA can be helpful in regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c32555",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((1000,2))\n",
    "\n",
    "X[:,0] = 4*np.random.randn(1000)\n",
    "X[:,1] = 2*X[:,0] + np.random.randn(1000)\n",
    "\n",
    "y = X[:,0] + 12 + X[:,1] + 1.5*np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e18b2",
   "metadata": {},
   "source": [
    "What is the correlation, if any, between the `0` and `1` columns of `X`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe58c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8d9a6",
   "metadata": {},
   "source": [
    "When features in $X$ are highly correlated with one another this is referred to as <i>multicolinearity</i>. Multicollinearity in a set of regression features can cause your coefficient estimates to have increased variance, and in general your estimates are not very stable to small changes in the model. Moreover, feature sets with high multicolinearity result in close to singular feature matrices, which can cause computational issues.\n",
    "\n",
    "PCA is one way to address multicolinearity because it takes your features and produces a new set of orthogonal features which have to be uncorrelated, because they are orthogonal.\n",
    "\n",
    "First make a scatter matrix that include the features contained in `X` and the target, `y`. Then run the feature matrix through PCA and plot the PCA projected feature matrix along with the target in a new scatter matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bcd84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026455a1",
   "metadata": {},
   "source": [
    "##### 5. P(olitical) Components Analysis\n",
    "\n",
    "Read in the data `Senate_115_roll_mat.csv` below. Note you will need to unzip this data file in your repository first.\n",
    "\n",
    "This data set contains the roll call votes for the 115th Senate. Each row represents a senator and the results of all of their roll call votes. The roll call votes are contained in columns labeled `1` through `599`.\n",
    "\n",
    "\n",
    "Run the vote columns through PCA with 2 components. Plot the fitted data colored by their `party_code`. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a86a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd79cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb7ef6",
   "metadata": {},
   "source": [
    "##### 6. Reducing MNIST\n",
    "\n",
    "We first introduce the MNIST data set. Load in the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6623d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "mnist = load_digits()\n",
    "\n",
    "X = mnist['data']\n",
    "y = mnist['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33764080",
   "metadata": {},
   "source": [
    "Each observation in this data set represents an 8x8 grayscale pixel grid of a handwritten digit from $0$-$9$. We visualize the first 10 observations below. Each pixel has a minimum value of $0$ and a maximum value of $255$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330cfc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "\n",
    "fig,ax = plt.subplots(2,5,figsize=(28,12))\n",
    "\n",
    "for i in range(10):\n",
    "    ax[i//5,i%5].imshow(X[i,:].reshape(8, 8), cmap='gray_r')\n",
    "    ax[i//5,i%5].text(.25,.25,str(y[i]),fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3826a",
   "metadata": {},
   "source": [
    "There is a lot of white space in these images. This indicates that we may be able to get away without needing all of the features.\n",
    "\n",
    "Look at the explained variance ratio. What seems like a good number of dimensions to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first scale the data\n",
    "X_scale = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1053dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798826bd",
   "metadata": {},
   "source": [
    "Now actually run `X` through PCA. Plot the first two components coloring the dots by their actual digit. Does it look like PCA does a good job of separating the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b769d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaadc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c5999",
   "metadata": {},
   "source": [
    "Now perform dimension reduction using Linear Discriminant Analysis instead. Plot the project down to the first two LDA directions, do you think this does a better job than the PCA at separating the data through a 2-D projection? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ec558",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12659efe",
   "metadata": {},
   "source": [
    "Now try applying `KernelPCA` with the radial basis function kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c71bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4dc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ffdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21390b2d",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba77c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
