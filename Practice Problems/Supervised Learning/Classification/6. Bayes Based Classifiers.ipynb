{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2670358d",
   "metadata": {},
   "source": [
    "# Bayes' Based Classifiers\n",
    "\n",
    "In this notebook will be some additional problems regarding LDA, QDA and naive Bayes. This material corresponds to `Lectures/Supervised Learning/Classification/6. Bayes Based Classifiers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1dae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce84e46",
   "metadata": {},
   "source": [
    "##### 1. LDA for supervised dimensionality reduction\n",
    "\n",
    "While we introduced linear discriminant analysis (LDA) as a classification algorithm, it was originally proposed by Fisher as a supervised dimension reduction technique, <a href=\"https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf\">https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf</a>. In particular, the initial goal was to project the features, $X$, corresponding to a binary output, $y$, onto a single dimension which best separates the possible classes. This single dimension has come to be known as <i>Fisher's discriminant</i>.\n",
    "\n",
    "In this \"problem\" you will learn how this works, and then reproduce Fisher's results on the iris data set (restricted to the versicolor and setosa classes) using `sklearn`'s `LinearDiscriminantAnalysis` model object.\n",
    "\n",
    "First load the Wisconsin cancer data set.\n",
    "\n",
    "<i>Note that we are not making a train test split here because we are not performing classification, just dimension reduction.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd20422",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "\n",
    "X = cancer['data']\n",
    "y = cancer['target']\n",
    "\n",
    "## changing labels on y\n",
    "y = -y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "\n",
    "X_scale = scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a22aa67",
   "metadata": {},
   "source": [
    "Using LDA for dimmensionality reduction we look for a vector, $w = (w_1, w_2, \\dots, w_m)^T$ with $||w||_2^2=1$, to project our features onto, i.e. find $Xw = w_1 X_1 + w_2 X_2 + \\dots w_m X_m$. We want to select $w$ in such a way that the spread between the two classes is maximized. In order to do that we first need to formally define what we mean by spread.\n",
    "\n",
    "First, if we let \n",
    "\n",
    "$$\n",
    "\\mu^{(c)} = \\frac{1}{N^{(c)}} \\sum_{i, y^{(i)}=c} X^{(i)}, \\text{ for } c=0,1\n",
    "$$\n",
    "\n",
    "where $N^{c}$ is the number of observations of class $c$, then\n",
    "\n",
    "$$\n",
    "\\tilde{\\mu}^{(c)} = \\frac{1}{N^{(c)}} \\sum_{i, y^{(i)}=c} X^{(i)} w = \\mu^{(c)} w, \\text{ for } c=0,1.\n",
    "$$\n",
    "\n",
    "That is, the mean for the projected data of either class is the projection of the class-specific mean vectors onto the vector $w$.\n",
    "\n",
    "Second, if $\\Sigma^{(c)}$ denotes the class $c$ specific covariance matrix of $X$, than the class $c$ specific variance of projection of $X$ onto $w$ is, $\\tilde{\\Sigma}^{(c)} = w^T \\Sigma^{(c)} w$.\n",
    "\n",
    "\n",
    "We can now define how we will measure the spread between the two classes in the projected space.\n",
    "\n",
    "We want to choose $w$ so that:\n",
    "\n",
    "$$\n",
    "S = \\frac{\\left( \\tilde{\\mu}^{(1)} - \\tilde{\\mu}^{(0)} \\right)^2}{\\tilde{\\Sigma}^{(1)} + \\tilde{\\Sigma}^{(0)}} = \\frac{\\left(\\mu^{(1)} w - \\mu^{(0)} w \\right)^2}{w^T \\left(\\Sigma^{(1)} + \\Sigma^{(0)} \\right) w}\n",
    "$$\n",
    "\n",
    "is as large as possible. $S$ may seem like an odd measure, but we can think of it as measuring how far apart the means are in comparison to the variance of the projected values. \n",
    "\n",
    "##### Why is this a useful measure?\n",
    "\n",
    "If $S>1$ that means the space between the means of the two class-specific distributions is larger than their combined variances. This suggests that we may be able to set a threshold, $\\delta$, such that for all observations with $y=0$ $Xw < \\delta$ and for all observations with $y=1$ $Xw > \\delta$.\n",
    "\n",
    "Doing some algebra and calculus it can be shown that the $w$ that maximizes $S$ is\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\left(\\Sigma^{(1)} + \\Sigma^{(0)}\\right)^{-1}\\left(\\mu^{(1)}  - \\mu^{(0)} \\right)\n",
    "$$\n",
    "\n",
    "##### Using `pandas` or `numpy` calculate $\\hat{w}$ for the cancer data. Then plot the histogram of $Xw$, colored by the class of the observation.\n",
    "\n",
    "<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.cov.html\">https://numpy.org/doc/stable/reference/generated/numpy.cov.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ac625",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10841614",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c5b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2174ef2",
   "metadata": {},
   "source": [
    "`sklearn`'s `LinearDiscriminantAnalysis` model object in addition to being a classification algorithm can also perform this supervised dimensionality reduction. To get the projected values call `LDA.transform(X)` after you fit the LDA model (this assumes that you called the model `LDA` and that your features are stored in `X_scale`.\n",
    "\n",
    "##### Use `sklearn`'s `LinearDiscriminantAnalysis` instead to perform LDA\n",
    "\n",
    "Note your results may look slightly different because `sklearn` does not implement the exact same Fisher discriminant calculation as you did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4849c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01184bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5edae",
   "metadata": {},
   "source": [
    "While Fisher formulated this approach with binary data in mind, the technique was extended to multiclass data by C.  R. Rao, <a href=\"https://www.jstor.org/stable/2983775\">https://www.jstor.org/stable/2983775</a>. If your data has $C$ possible classes, the multiclass approach will project the data down to a space of dimension at most $C-1$. This too can be implemented by `sklearn`, where you can control the number of components using `n_components` as an argument when you make the `LinearDiscriminantAnalysis` model object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f1e14",
   "metadata": {},
   "source": [
    "##### 2. Multi-class LDA dimension reduction\n",
    "\n",
    "Load the iris data set and perform LDA dimension reduction. Plot the projected points, coloring by iris type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5180ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ba08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8751e5",
   "metadata": {},
   "source": [
    "##### 3. Return to cancer data\n",
    "\n",
    "Return to the cancer data set from 1. This time make a train test split, then make a pipeline that scales the data and ends in LDA.\n",
    "\n",
    "Instead of using the LDA predictions, we will loop through possible Fisher discriminant cutoff values for classification. That is perform 5-fold cross-validation to check which value of $c$ provides the best TPR, FPR and precision where $c$ is such that $\\text{Fisher discriminant } < c$ is classified as benign ($y = 0$) and a discriminat greater than or equal to $c$ is classified as malignant ($y=1$).\n",
    "\n",
    "Plot the avg. cv. TPR, FPR and precision as a function of $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer['data']\n",
    "y = cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55db811",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e2909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8537fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584886cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e2d9f",
   "metadata": {},
   "source": [
    "##### 4. Spam Classification\n",
    "\n",
    "One of the initial uses for the na&#xEF;ve Bayes classifier was as an email spam detection algorithm. In this problem you will build such a model.\n",
    "\n",
    "The data contained in `SMSSpamCollection.tsv` contains $5{,}572$ SMS messages that have been classified as either a spam message or a ham message (aka not spam).\n",
    "\n",
    "Read in the file, `SMSSpamCollection.tsv` from the `Data` file, then make a `target` column that is `0` if the observation has an `outcome` column value of `\"ham\"` and a `1` if it is `\"spam\"`. Finally make a train test split stratified on the `target` column.\n",
    "\n",
    "<i>Note: Some of the messages may include inappropriate language</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_table(\"../../../Data/SMSSpamCollection.tsv\")\n",
    "\n",
    "spam['target'] = 0\n",
    "spam.loc[spam.outcome=='spam','target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c314ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01542b",
   "metadata": {},
   "source": [
    "Using the training set, what is the split between spam and ham messages? Again using the training set to estimate, what would we expect a model that classifies everything as `\"ham\"` to have as its accuracy, true positive rate and false positive rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bd3f0",
   "metadata": {},
   "source": [
    "The data we will use to build our model are $50$ indicator variables that tell you whether or not a message contains a particular word. For example the column `good` would be `1` if the message contains the word \"good\" and `0` otherwise.\n",
    "\n",
    "Use `BernoulliNB` from `sklearn.naive_bayes`, <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes\">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes</a> to build the na&#xEF;ve Bayes classifier on these data. Find the average cross-validation accuracy, TPR and FPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a51aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15883ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223be83b",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086f22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
