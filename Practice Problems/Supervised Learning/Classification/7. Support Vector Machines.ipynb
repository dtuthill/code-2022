{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5d14eb",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "In this notebook will be some additional problems regarding SVMs. This material corresponds to `Lectures/Supervised Learning/Classification/8. Support Vector Machines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bdc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff805eb",
   "metadata": {},
   "source": [
    "##### 1. Probability from SVMs (Not a true \"problem\")\n",
    "\n",
    "In `LinearSVC` or `SVC` we can set `probability=True` in order to produce a support vector machine that can also provide a probability that each observation is of class $1$. A reasonable question for you to have is, how is that probability produced?\n",
    "\n",
    "`sklearn` uses an approach developed by John C. Platt laid out in this paper <a href=\"http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=A825923D4C5A7E8CE0683856A9C32AF6?doi=10.1.1.41.1639&rep=rep1&type=pdf\">http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=A825923D4C5A7E8CE0683856A9C32AF6?doi=10.1.1.41.1639&rep=rep1&type=pdf</a>. We will give the basic idea behind this approach, but for full details read that paper.\n",
    "\n",
    "Recall that the classification decision for a support vector machine involves calculating a decision function $f(X)$, (For a linear support vector machine this is the value of the scalar projection of the point $X$ onto the separating hyperplane). In order to produce an estimate of $P(y=1|X)$ we instead model $P(y=1|f(X))$ like so:\n",
    "\n",
    "$$\n",
    "P(y=1|f(X)) = \\frac{1}{1+\\exp\\left(Af(X) + B\\right)}.\n",
    "$$\n",
    "\n",
    "The parameters $A$ and $B$ are fit using maximum likelihood methods combined with a holdout set or cross-validation. For more in depth details see the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7011006",
   "metadata": {},
   "source": [
    "##### 2. Multiclass SVMs\n",
    "\n",
    "When we formulated support vector machines in lecture we presented them as a binary classification algorithm. However, support vector machines can also be used for multiclass classification.\n",
    "\n",
    "`sklearn` approaches multiclass support vector machines with a one vs one approach. In the one vs. one approach for each possible pair of classes you train a unique support vector machine classifier. For example, if you have three classes, $1,2,3$, you would train a:\n",
    "- 1 or 2 classifier,\n",
    "- 1 or 3 classifier, and a\n",
    "- 2 or 3 classifier.\n",
    "\n",
    "A prediction is then made by choosing the class that is predicted the most among all of the classifiers.\n",
    "\n",
    "For a problem with $\\mathcal{C}$ possible classes, how many support vector machines are trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960bf15",
   "metadata": {},
   "source": [
    "##### Write here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fda7d9",
   "metadata": {},
   "source": [
    "##### 3. Hinge Loss\n",
    "\n",
    "Recall that for linear support vector machines predicting a variable $y\\in\\left\\lbrace-1,1\\right\\rbrace$ using $m$ features stored in $X$ we were concerned with fitting a hyperplane:\n",
    "\n",
    "$$\n",
    "f(X) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_m X_m,\n",
    "$$\n",
    "\n",
    "under some constraints.\n",
    "\n",
    "In the case of the support vector classification restraints, this can more concisely be written as a single minimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta_0, \\dots, \\beta_m} \\left\\lbrace \\sum_{i=1}^n\\max\\left[0, 1-y^{(i)} f\\left(X^{(i)}\\right) \\right] + \\lambda \\sum_{j=1}^m \\beta_j^2 \\right\\rbrace,\n",
    "$$\n",
    "\n",
    "where $(X^{(i)},y^{(i)})$ denotes the $i^\\text{th}$ observation of $n$ total training observations.\n",
    "\n",
    "This form is similar to that of Lasso and Ridge regression where we look to minimize:\n",
    "\n",
    "$$\n",
    "L(X,y,\\beta) + \\lambda P(\\beta)\n",
    "$$\n",
    "\n",
    "as a function of $\\beta$. Here $L$ denotes a loss function and $P$ a penalty term. In regression the loss function is the MSE and the penalty is either $||\\beta||_1$ or $||\\beta||_2^2$. For support vector classifiers the loss function is known as the <i>hinge loss</i> and the penalty is again $||\\beta||_2^2$.\n",
    "\n",
    "Explicity the hinge loss is defined as:\n",
    "\n",
    "$$\n",
    "L(X,y,\\beta) = \\sum_{i=1}^n\\max\\left[0, 1-y^{(i)} f\\left(X^{(i)}\\right) \\right].\n",
    "$$\n",
    "\n",
    "From this we can see that only the observations on the incorrect side of the margin impact the loss function, for those are precisely the observations with $y^{(i)} f\\left(X^{(i)}\\right) < 1$. Below we plot $\\max\\left[0, 1-y^{(i)} f\\left(X^{(i)}\\right) \\right]$ as a function  of $y^{(i)} f\\left(X^{(i)}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95619853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge(x):\n",
    "    return np.max([0, 1-x])\n",
    "\n",
    "xs = np.linspace(-2,3,100)\n",
    "hinges = [hinge(x) for x in xs]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(xs, hinges)\n",
    "\n",
    "plt.xlabel(\"$y^{(i)} f(X^{(i)})$\", fontsize=16)\n",
    "plt.ylabel(\"Hinge Loss Term\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9564c14",
   "metadata": {},
   "source": [
    "What happens to the values of $\\beta_1, \\dots, \\beta_m$ as you increase or decrease the value of $\\lambda$ in the minimization problem above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822cc2d",
   "metadata": {},
   "source": [
    "##### Write here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c05706",
   "metadata": {},
   "source": [
    "##### 4. Support Vector Regression\n",
    "\n",
    "We can also use support vector machines to solve regression problems. I will use two dimensions to display the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c6716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(614)\n",
    "X = np.random.random(30)\n",
    "y = X + .25*np.random.randn(30)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X,y)\n",
    "plt.plot([0,1], [0,1], 'k', label=\"$\\hat{f}(X)$\")\n",
    "plt.plot([0,1], [.4,1.4], 'k--', label=\"$\\hat{f}(X) \\pm\\\\xi$\")\n",
    "plt.plot([0,1], [-.4,.6], 'k--')\n",
    "\n",
    "for i in range(30):\n",
    "    if np.abs(y[i]-X[i])>.4:\n",
    "        plt.scatter(X[i], y[i], c='r', s=150)\n",
    "\n",
    "plt.plot([.2,.2], [.2,.6],'r--')\n",
    "plt.plot([.2,.2], [.2,-.2],'r--')\n",
    "\n",
    "plt.text(.21,.4,\"$\\\\xi$\", fontsize=18)\n",
    "plt.text(.21,0,\"$\\\\xi$\", fontsize=18)\n",
    "\n",
    "plt.xlabel(\"$X$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537660c5",
   "metadata": {},
   "source": [
    "Where we let $f(X) = \\beta_0 + \\beta_1 X$.\n",
    "\n",
    "We now use this plot to help describe the idea behind support vector regression. <i>we will be doing this for estimating a linear relationship between $y$ and $X$, but we can similarly use the kernel trick to estimate nonlinear relationships between $y$ and $X$ as well.</i>\n",
    "\n",
    "Recall that for regression we want to predict a continuous variable $y$ using a set of features $X$. In particular we want to find $f$ such that $y=f(X)+\\epsilon$ for some error $\\epsilon$. For a linear relationship this takes the form of:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_m X_m + \\epsilon.\n",
    "$$\n",
    "\n",
    "We want to estimate the $\\beta_i$. In support vector regression we essentially estimate by wiggling around a hyperplane to get $\\hat{y}$ within $\\xi$ of $y$ for as many training points as possible for a value of $\\xi$ that you choose prior to fitting the model. This is visualized in the plot above for a single feature, $X$. We have some line fit to these data that is within $\\xi$ of all but $3$ training points. In this example, imagine trying to rotate the solid line so that we fit as many of the training points as possible between the dotted lines. However, if we are allowed to wiggle our hyperplane around as much as we would like, we would inevitably overfit on the training data, so we have a constraint on how much we are allowed to wiggle the hyperplane. \n",
    "\n",
    "Formally we estimate the $\\beta_i$ by minimizing:\n",
    "\n",
    "$$\n",
    "H(\\beta_0, \\beta_1, \\dots, \\beta_m|\\xi, \\lambda) = \\sum_{i = 1}^n \\max \\left[ 0, | y^{(i)} - f\\left(X^{(i)}\\right) | - \\xi \\right] + \\lambda \\sum_{i=1}^m \\beta_i^2,\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a constant you choose priort to fitting the model, $f(X) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_m X_m$ and the superscripts denote observations in the training set, which again takes the form of a loss function plus a penalty term. We can see that the only terms that contribute to the loss portion of $H$ are those for which $f\\left(X^{(i)}\\right)$ is further than $\\xi$ away from $y^{(i)}$. Thus \"we\" can reduce the value of $H$ in two ways:\n",
    "\n",
    "1. Move the hyperplane (change the value on the $\\beta_i$) so that more predictions are within $\\xi$ of their training observations,\n",
    "\n",
    "2. Make the $\\beta_i$ smaller in magnitude.\n",
    "\n",
    "Which path your computer takes can be determined by the values of $\\xi$ and $\\lambda$. For more detailed information on how this model is fit see the Elements of Statistical Learning Chapter 12, <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/>https://web.stanford.edu/~hastie/ElemStatLearn/</a>.\n",
    "\n",
    "<i>Fill in the blank questions</i>\n",
    "\n",
    "For a fixed value of $\\lambda$ increasing the value of $\\xi$ <i>$<$blank$>$</i> the variance of the model and <i>$<$blank$>$</i> the bias of the model. For a fixed value of $\\lambda$ decreasing the value of $\\xi$ <i>$<$blank$>$</i> the variance of the model and <i>blank</i> the bias of the model.\n",
    "\n",
    "For a fixed value of $\\xi$ increasing the value of $\\lambda$ <i>$<$blank$>$</i> the variance of the model and <i>blank</i> the bias of the model. For a fixed value of $\\xi$ decreasing the value of $\\lambda$ <i>$<$blank$>$</i> the variance of the model and <i>blank</i> the bias of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df892d7",
   "metadata": {},
   "source": [
    "##### Write here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7d238",
   "metadata": {},
   "source": [
    "##### 5. Moons and Circles Data Set\n",
    "\n",
    "Two popular benchmark data sets are the <i>moons</i> and <i>circles</i> datasets that are plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de79822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae859c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Moon Data set\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\n",
    "X_moons,y_moons = make_moons(1000, noise=.2)\n",
    "\n",
    "## Circles Data Set\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html\n",
    "X_circ,y_circ = make_circles(1000, noise=.2, factor=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.scatter(X_moons[y_moons==0,0], X_moons[y_moons==0,1], c='blue', alpha=.7, label=\"y=0\")\n",
    "plt.scatter(X_moons[y_moons==1,0], X_moons[y_moons==1,1], c='orange', marker='v', alpha=.7, label=\"y=1\")\n",
    "\n",
    "plt.xlabel(\"$X_1$\", fontsize=14)\n",
    "plt.ylabel(\"$X_2$\", fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Moons Data Set\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.scatter(X_circ[y_circ==0,0], X_circ[y_circ==0,1], c='blue', alpha=.7, label=\"y=0\")\n",
    "plt.scatter(X_circ[y_circ==1,0], X_circ[y_circ==1,1], c='orange', marker='v', alpha=.7, label=\"y=1\")\n",
    "\n",
    "plt.xlabel(\"$X_1$\", fontsize=14)\n",
    "plt.ylabel(\"$X_2$\", fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Circle Data Set\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2725c7c",
   "metadata": {},
   "source": [
    "Explore different kernel functions for the `SVC` classifier and find a good one for both data sets.\n",
    "\n",
    "<i>If you attempt to use a polynomial kernel you may want to adjust arguments like `coef0` and `degree`.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af267547",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a570cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da758061",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1eb93",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb824300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
