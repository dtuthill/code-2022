{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e817a66",
   "metadata": {},
   "source": [
    "# Non-Boosting Methods\n",
    "\n",
    "In this notebook will be some additional problems regarding non-boosting ensemble learning methods. This material corresponds to lectures:\n",
    "- `Lectures/Supervised Learning/Ensemble Learning/1. What is Ensemble Learning`,\n",
    "- `Lectures/Supervised Learning/Ensemble Learning/2. Random Forests`,\n",
    "- `Lectures/Supervised Learning/Ensemble Learning/3. Bagging and Pasting` and\n",
    "- `Lectures/Supervised Learning/Ensemble Learning/8. Voter Models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e81e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d8898",
   "metadata": {},
   "source": [
    "##### 1. \n",
    "\n",
    "If you have trained five different models on the exact same training data, and they all achieve $95\\%$ precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed88a3",
   "metadata": {},
   "source": [
    "##### Write here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be76bb",
   "metadata": {},
   "source": [
    "##### 2. Voter Model Regression\n",
    "\n",
    "While we implemented a voter model for a classification problem, it can also be used for regression purposes. In this setting the voter model provides a predicted value by taking a weighted average (the default is to use uniform weights) of its constituent regressor model's predictions.\n",
    "\n",
    "In `sklearn` this is done with `VotingRegressor` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html</a>.\n",
    "\n",
    "Load in the `baseball_run_diff.csv` data set from the `Data` folder. Build a `VotingRegressor` model to predict `W` using `RD`. Let your constituent models be simple linear regression, $k$-nearest neighbors using $k=10$ and an extra trees regressor with `max_depth=4`. Plot the predictions on top of the training data for the voter model as well as each individual constituent model. In addition create a validation set and provide the performance of the voter model and each individual constituent model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ef7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the data here\n",
    "baseball = pd.read_csv(\"../../../Data/baseball_run_diff.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7dbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "baseball_train, baseball_test = train_test_split(baseball.copy(), \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=314,\n",
    "                                                    test_size=.2)\n",
    "\n",
    "## make a validation set\n",
    "baseball_train_train, baseball_val = train_test_split(baseball_train.copy(), \n",
    "                                                        shuffle=True,\n",
    "                                                        random_state=13241,\n",
    "                                                        test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65492ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa074a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e51699",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d415fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd056721",
   "metadata": {},
   "source": [
    "##### 3. Bagging/Pasting Regression\n",
    "\n",
    "Similarly to 2., we can use bagging/pasting models for regression as well. Here we use a regression model as our base estimator and then to make a prediction take an average of all `n_estimators` models' predictions. In `sklearn` this is performed with `BaggingRegressor`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html</a>, where just like `BaggingClassifier` whether you perform bagging or pasting is determined by `bootstrap`.\n",
    "\n",
    "Build a bagging regression model on that baseball data using a $k$NN regressor with $k=5$ as the base estimator. Plot the training data, the $k$NN regression prediction and the bagging prediction on the same plot. Use `n_estimators=5`, `bootstrap=True` and `max_samples = int(.25*len(baseball_train))`.\n",
    "\n",
    "\n",
    "<i>Would using a bagging regressor introduce more bias or variance to your model?</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45489fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f299532",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ceebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b37c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c3bd2",
   "metadata": {},
   "source": [
    "##### 4. Introducing MNIST\n",
    "\n",
    "The MNIST dataset is a database of $60,000$ handwritten digits. It has been used to help create computer vision algorithms in detecting handwritten digits. It is also a common dataset used when teaching classification. \n",
    "\n",
    "We will first go through the data together, then you will build a voting classifier on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data, this can take a while\n",
    "## For speed, we'll use the sklearn version of the data\n",
    "## which is smaller data set, and has a lower resolution\n",
    "## than the original data set\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X,y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8eccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each observation contains the grayscale values for an 8 x 8 grid\n",
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec6f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "\n",
    "fig,ax = plt.subplots(2,5,figsize=(20,8))\n",
    "\n",
    "for i in range(10):\n",
    "    ax[i//5,i%5].imshow(X[i,:].reshape(8, 8), cmap='gray_r')\n",
    "    ax[i//5,i%5].text(.1,.1,str(y[i]),fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0aaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I'll scale X for you prior to the split, this ensures\n",
    "## each value is in the range of 0 to 1\n",
    "X = X/255\n",
    "\n",
    "## Perform the split here\n",
    "## set aside 20% for the test set\n",
    "## stratify on y\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,\n",
    "                                                    test_size = .2,\n",
    "                                                    shuffle = True,\n",
    "                                                    random_state=440,\n",
    "                                                    stratify=y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fd3b49",
   "metadata": {},
   "source": [
    "Use cross-validation to find the number of neighbors for a KNN classifier that produces the highest mean cv accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ebbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330bdc9",
   "metadata": {},
   "source": [
    "Perform cross-validation to select the optimal value for `max_depth` for a `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97011809",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907bea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea6384",
   "metadata": {},
   "source": [
    "Now build a voter model with KNN using the number of neighbors you just found, a random forest model using the maximum depth you just found and a linear discriminant analysis model. Find the cross-validation accuracy for the voter model and each model individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f626dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56043af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83654b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8bb6a3",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d0158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
