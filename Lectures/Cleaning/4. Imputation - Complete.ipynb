{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9b68bb",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "\n",
    "Sometimes our features may be missing observations, what can we do.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss a method to deal with missing values,\n",
    "- Demonstrate that method on a penguin data set,\n",
    "- Illustrate various approaches to imputation and\n",
    "- Show how to integrate imputation into a train test split procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472ffc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seaborn import set_style\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a905250",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "There will be times when you need to work with a data set that has missing data. We can see one such example with this edited `seaborn` penguins data set, <a href=\"https://seaborn.pydata.org/examples/scatterplot_matrix.html\">https://seaborn.pydata.org/examples/scatterplot_matrix.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738558c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"../../Data/penguins_w_nas.csv\")\n",
    "\n",
    "penguins.loc[penguins.sex=='Male', 'sex'] = 0\n",
    "penguins.loc[penguins.sex=='Female', 'sex'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80868262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 344 entries, 0 to 343\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            344 non-null    object \n",
      " 1   bill_length_mm     342 non-null    float64\n",
      " 2   bill_depth_mm      341 non-null    float64\n",
      " 3   flipper_length_mm  342 non-null    float64\n",
      " 4   body_mass_g        339 non-null    float64\n",
      " 5   sex                333 non-null    object \n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 16.2+ KB\n"
     ]
    }
   ],
   "source": [
    "penguins.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae74d2",
   "metadata": {},
   "source": [
    "Using `.info()` above we can see that the data set has 344 entries with some observations that are missing values for one or more columns. If there are NAs in features that you plan to use in a model you will be unable to use those observations in training or validation sets.\n",
    "\n",
    "So what can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27bf175",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "The process of replacing missing values in data is known as <i>imputation</i>. There are a few different ways you can impute missing data.\n",
    "\n",
    "### Imputing a preset constant value\n",
    "\n",
    "The simplest approach is just just impute a constant value. For example, maybe scientists have already observed an average body mass for penguins, let's say it is `4207`. You could then impute this value for all missing `body_mass_g` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92faf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a copy to demonstrate impute strategy\n",
    "penguins_constant_impute = penguins.copy()\n",
    "\n",
    "## Replace the missing data\n",
    "## .isna() checks for missing data\n",
    "penguins_constant_impute.loc[penguins_constant_impute.body_mass_g.isna(), 'body_mass_g'] = 4207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242d35fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 344 entries, 0 to 343\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            344 non-null    object \n",
      " 1   bill_length_mm     342 non-null    float64\n",
      " 2   bill_depth_mm      341 non-null    float64\n",
      " 3   flipper_length_mm  342 non-null    float64\n",
      " 4   body_mass_g        344 non-null    float64\n",
      " 5   sex                333 non-null    object \n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 16.2+ KB\n"
     ]
    }
   ],
   "source": [
    "penguins_constant_impute.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8075466",
   "metadata": {},
   "source": [
    "### Imputing based on a sample statistic\n",
    "\n",
    "A common strategy is to impute missing values using sample statistics from the non-missing values in a column. For example, we can replace NAs with the mean, median or mode of the column.\n",
    "\n",
    "While we can do this by hand using `numpy` or `pandas` it may be easier, and more easily used in predictive modeling, to use `sklearn`'s `SimpleImputer`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\">https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "173ce1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "429015c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4050.],\n",
       "       [4050.],\n",
       "       [4050.],\n",
       "       [4050.],\n",
       "       [4050.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make a SimpleImputer object\n",
    "## strategy determines how the imputation happens\n",
    "## options are 'mean', 'median', 'most_frequent' and 'constant'\n",
    "impute = SimpleImputer(strategy = 'median')\n",
    "\n",
    "## fit the impute object\n",
    "impute.fit(penguins['body_mass_g'].values.reshape(-1,1))\n",
    "\n",
    "## Show the transformed data\n",
    "impute.transform(penguins['body_mass_g'].values.reshape(-1,1))[penguins.body_mass_g.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6aca894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4050.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.body_mass_g.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf74227",
   "metadata": {},
   "source": [
    "### Building a model to impute\n",
    "\n",
    "We can also build a model to impute the missing values. Let's regress `body_mass_g` on all other columns to get imputed values for the missing data.\n",
    "\n",
    "Note that we cannot do this for observations missing multiple features. For now we will ignore those rows, but in practice we would have to come up for unique imputation strategies for each columns (and sometimes each row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a57e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>38.2</td>\n",
       "      <td>18.1</td>\n",
       "      <td>185.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.8</td>\n",
       "      <td>188.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>43.3</td>\n",
       "      <td>13.4</td>\n",
       "      <td>209.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "3    Adelie             NaN            NaN                NaN          NaN   \n",
       "23   Adelie            38.2           18.1              185.0          NaN   \n",
       "32   Adelie            39.5           17.8              188.0          NaN   \n",
       "228  Gentoo            43.3           13.4              209.0          NaN   \n",
       "339  Gentoo             NaN            NaN                NaN          NaN   \n",
       "\n",
       "     sex  \n",
       "3    NaN  \n",
       "23     0  \n",
       "32     1  \n",
       "228    1  \n",
       "339  NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.loc[penguins.body_mass_g.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acd46452",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_na = penguins.loc[penguins.body_mass_g.isna()].dropna(subset=['bill_length_mm', \n",
    "                                                                       'bill_depth_mm', \n",
    "                                                                       'flipper_length_mm', \n",
    "                                                                       'sex']).copy()\n",
    "penguins_non_na = penguins.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e67917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_na[['Adelie', 'Gentoo']] = pd.get_dummies(penguins_na.species)[['Adelie', 'Gentoo']].copy()\n",
    "penguins_non_na[['Adelie', 'Gentoo']] = pd.get_dummies(penguins_non_na.species)[['Adelie', 'Gentoo']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f10e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c360310d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression(copy_X=True)\n",
    "\n",
    "reg.fit(penguins_non_na[['bill_length_mm', \n",
    "                         'bill_depth_mm', \n",
    "                         'flipper_length_mm', \n",
    "                         'sex', \n",
    "                         'Adelie', \n",
    "                         'Gentoo']].values,\n",
    "        penguins_non_na.body_mass_g.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2561f21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3789.59016416, 3455.21140107, 4578.58391692])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(penguins_na[['bill_length_mm', \n",
    "                         'bill_depth_mm', \n",
    "                         'flipper_length_mm', \n",
    "                         'sex', \n",
    "                         'Adelie', \n",
    "                         'Gentoo']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9d43f",
   "metadata": {},
   "source": [
    "For missing values that occur in rows where modeling is impossible you would have to use one of the other prior strategies.\n",
    "\n",
    "#### A note about predictive modeling\n",
    "\n",
    "If the end goal for your data set is to build a predictive model you <b>cannot</b> use the column that you are trying to predict in an imputation model. For example, if we were looking to build a model that predicted the `species` of the penguin we would not be allowed to use it for feature imputation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9554cf",
   "metadata": {},
   "source": [
    "## Imputation in predictive modeling projects\n",
    "\n",
    "Recall that when we scale data in predictive modeling projects we have to fit the scaler on the training data and then use that fit scaler on the test data. Importantly, we do <b>not</b> refit the scaler on the test or holdout data. We have to take a similar approach to any imputation technique.\n",
    "\n",
    "For example:\n",
    "- Imputation using any sample statistic approach must use the sample statistic computed on the training data to impute missing values in the test, validation or holdout sets and\n",
    "- Models trained to impute missing values must be fit on the training data and that fitted model is what used on the test data.\n",
    "\n",
    "`sklearn`'s `SimpleImputer` object is nicely set up for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1b653f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14447d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_train, penguins_test = train_test_split(penguins, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=32,\n",
    "                                                    test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b188f65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 275 entries, 180 to 215\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            275 non-null    object \n",
      " 1   bill_length_mm     275 non-null    float64\n",
      " 2   bill_depth_mm      274 non-null    float64\n",
      " 3   flipper_length_mm  275 non-null    float64\n",
      " 4   body_mass_g        273 non-null    float64\n",
      " 5   sex                266 non-null    object \n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 15.0+ KB\n"
     ]
    }
   ],
   "source": [
    "penguins_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a17867a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 69 entries, 266 to 197\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            69 non-null     object \n",
      " 1   bill_length_mm     67 non-null     float64\n",
      " 2   bill_depth_mm      67 non-null     float64\n",
      " 3   flipper_length_mm  67 non-null     float64\n",
      " 4   body_mass_g        66 non-null     float64\n",
      " 5   sex                67 non-null     object \n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 3.8+ KB\n"
     ]
    }
   ],
   "source": [
    "penguins_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define an imputer with the 'mean' strategy\n",
    "impute = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "\n",
    "\n",
    "## fitting the imputer on the training set\n",
    "impute.fit(penguins_train[['bill_length_mm', \n",
    "                          'bill_depth_mm', \n",
    "                          'flipper_length_mm',\n",
    "                          'body_mass_g']])\n",
    "\n",
    "## Imputing the training set\n",
    "impute.transform(penguins_train[['bill_length_mm', \n",
    "                              'bill_depth_mm', \n",
    "                              'flipper_length_mm',\n",
    "                              'body_mass_g']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6e389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Imputing the test set\n",
    "impute.transform(penguins_test[['bill_length_mm', \n",
    "                          'bill_depth_mm', \n",
    "                          'flipper_length_mm',\n",
    "                          'body_mass_g']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94f2ca",
   "metadata": {},
   "source": [
    "`SimpleImputer` (and `sklearn`'s other imputer objects, <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute\">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute</a>) can be implemented into a pipeline just like `StandardScaler` or any other `sklearn` preprocessing object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75f670",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc388608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
