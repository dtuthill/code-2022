{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72d4578",
   "metadata": {},
   "source": [
    "# More Advanced Pipelines\n",
    "\n",
    "Sometimes you will need more customized functionality in your pipeline.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Return to our penguins data set example,\n",
    "- See how to build custom transformer objects,\n",
    "- Introduce `FunctionTransformer`,\n",
    "- Again review the `fit`, `transform` and `fit_transform` paradigm and\n",
    "- Build a pipeline to predict penguin type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeec829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seaborn import set_style, pairplot\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f2f94",
   "metadata": {},
   "source": [
    "## Penguins Data\n",
    "\n",
    "A motivating problem for this notebook will be our desire to build a model that classifies different penguins into three unique species. We saw a version of this `seaborn` data set in the `Imputation` notebook. \n",
    "\n",
    "Let's load the version we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c417ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2771c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"../../Data/penguins_for_pipes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fa3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc47f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_train, peng_test = train_test_split(penguins,\n",
    "                                            random_state = 440,\n",
    "                                            shuffle = True,\n",
    "                                            test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot(peng_train, hue='species')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peng_train.island.value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print(peng_train.sex.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a61df40",
   "metadata": {},
   "source": [
    "## Our desired pipeline\n",
    "\n",
    "Our desired pipeline for this model is:\n",
    "\n",
    "1. Impute the missing values of `body_mass_g` with the `median` value,\n",
    "2. Impute the missing values of `sex` with the most common value,\n",
    "3. One hot encode `island` and `sex` and\n",
    "4. Fit a random forest model to the data.\n",
    "\n",
    "The reason that our basic pipeline approach may face issues here is that we are mixing data with categorical and numeric features. For example, just calling `SimpleImputer` in a pipeline will cause the same imputation to be applied to all missing values regardless of column. Another time this could be an issue is if you want to scale some columns but not others.\n",
    "\n",
    "## Transformer objects\n",
    "\n",
    "`sklearn` has the functionality for us to write our own custom <i>transformer objects</i> which we can use to make custom imputers. A transformer object in this context is an `sklearn` object that performs some transformation on the data. We have seen a number of these in prior notebooks including:\n",
    "- `StandardScaler`,\n",
    "- `PolynomialFeatures` and\n",
    "- `SimpleImputer`.\n",
    "\n",
    "### Making custom trasformer objects\n",
    "\n",
    "We can now learn how to make our own transformer objects.\n",
    "\n",
    "We will start by making a custom imputer for `body_mass_g`.\n",
    "\n",
    "<i>Note that this material may be more easily digested if you have consumed the optional `Classes and Objects in Python` notebook in the `Python Prep` material in this repository.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faeb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll need these\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our custom imputer\n",
    "class BodyMassImputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call\n",
    "    # BodyMassImputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with\n",
    "        # the SimpleImputer method\n",
    "        self.SimpleImputer = SimpleImputer(strategy = \"median\")\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\"\n",
    "    # SimpleImputer's fit method using only the\n",
    "    # 'body_mass_g' column\n",
    "    def fit(self, X, y = None ):\n",
    "        self.SimpleImputer.fit(X['body_mass_g'].values.reshape(-1,1))\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the 'body_mass_g' columns\n",
    "    # and return it with imputed values\n",
    "    def transform(self, X, y = None):\n",
    "        copy_X = X.copy()\n",
    "        copy_X['body_mass_g'] = self.SimpleImputer.transform(copy_X['body_mass_g'].values.reshape(-1,1))\n",
    "        return copy_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146ddc1",
   "metadata": {},
   "source": [
    "We now have a custom imputer let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e288ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = BodyMassImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70745ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the data where body_mass_g is missing\n",
    "peng_train.loc[peng_train.body_mass_g.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is what results at the missing values of body_mass_g in the train set\n",
    "## note we can use fit_transform because it is the training set\n",
    "imputer.fit_transform(peng_train).loc[peng_train.body_mass_g.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1739ca",
   "metadata": {},
   "source": [
    "We can make a custom imputer for `sex` in a similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50616b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our custom imputer\n",
    "class SexImputer(BaseEstimator, TransformerMixin):\n",
    "    # Class Constructor \n",
    "    # This allows you to initiate the class when you call\n",
    "    # SexImputer\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with\n",
    "        # the SimpleImputer method\n",
    "        \n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\"\n",
    "    # SimpleImputer's fit method using only the\n",
    "    # 'sex' column\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now I want to transform the 'sex' columns\n",
    "    # and return it with imputed values\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcfedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SexImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281eeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_train.loc[peng_train.sex.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit_transform(peng_train).loc[peng_train.sex.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2388763",
   "metadata": {},
   "source": [
    "Finally we need a one hot encoder for these data. Luckily we can use a `FunctionTransformer` object for this, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html</a>.\n",
    "\n",
    "`FunctionTransformer` allows you to write a python function and have it applied to your data set as a transformer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588d7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f1e6d8e",
   "metadata": {},
   "source": [
    "First we define a function that will take in the dataframe and return one with one hot encoded data for `island` and `sex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    ## first replace Male Female with 0-1s\n",
    "    df_copy['sex'] = pd.get_dummies(df['sex'])['Female'].copy()\n",
    "    \n",
    "    ## Now get island columns\n",
    "    df_copy[['Biscoe', 'Dream', 'Torgersen']] = pd.get_dummies(df['island'])[['Biscoe', 'Dream', 'Torgersen']]\n",
    "    \n",
    "    return df_copy[['bill_length_mm', 'bill_depth_mm',\n",
    "               'flipper_length_mm', 'body_mass_g', \n",
    "               'sex', 'Biscoe', 'Dream', 'Torgersen']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c917b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the one hot encoded data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285c845",
   "metadata": {},
   "source": [
    "We can now wrap the function `one_hot_encoder` in the `FunctionTransformer` object to turn it into a transformer object that does the one hot encoding we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_transformer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac714e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_transformer.transform(peng_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58eb79d",
   "metadata": {},
   "source": [
    "Note that with `one_hot_transformer` we did not have to call `fit` prior to `transform` this is because `FunctionTransformer` does not need to be fitted, it is just applying the function we wrote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c77e4",
   "metadata": {},
   "source": [
    "### Making the pipeline\n",
    "\n",
    "We can now put all of this together in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7fdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(peng_train[['bill_length_mm', 'bill_depth_mm',\n",
    "                       'flipper_length_mm', 'body_mass_g',\n",
    "                       'island', 'sex']],\n",
    "         peng_train['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pipe.predict(peng_train[['bill_length_mm', 'bill_depth_mm',\n",
    "                       'flipper_length_mm', 'body_mass_g',\n",
    "                       'island', 'sex']])\n",
    "\n",
    "pipe.predict(peng_train[['bill_length_mm', 'bill_depth_mm',\n",
    "                       'flipper_length_mm', 'body_mass_g',\n",
    "                       'island', 'sex']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipe.predict(peng_test[['bill_length_mm', 'bill_depth_mm',\n",
    "                       'flipper_length_mm', 'body_mass_g',\n",
    "                       'island', 'sex']])\n",
    "\n",
    "pipe.predict(peng_test[['bill_length_mm', 'bill_depth_mm',\n",
    "                       'flipper_length_mm', 'body_mass_g',\n",
    "                       'island', 'sex']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ef72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbf53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy\", accuracy_score(peng_train.species, train_pred))\n",
    "print(\"Test accuracy\", accuracy_score(peng_train.species, train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193a1456",
   "metadata": {},
   "source": [
    "You should now be able to create more complicated pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154e288",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272143a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
