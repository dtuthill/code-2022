{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd2c672",
   "metadata": {},
   "source": [
    "# Scaling Data\n",
    "\n",
    "Sometimes prior to fitting a model or running an algorithm we will need to scale our data. This is particularly true when some features (columns of $X$) are on vastly different scales than others. In this notebook we will demonstrate how to scale.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the concept of scaling your data,\n",
    "- Demonstrate the `StandardScaler` object in `sklearn`,\n",
    "- Discuss `fit`, `transform` and `fit_transform` and\n",
    "- Show the scaling process using `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ac0a8",
   "metadata": {},
   "source": [
    "Before we get started let's generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b38485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bf06094",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make some data\n",
    "## Notice that the columns of X have vastly different scales\n",
    "X = np.zeros((1000,4))\n",
    "X[:,0] = 1000*np.random.randn(1000)\n",
    "X[:,1] = np.random.random(1000) - 10\n",
    "X[:,2] = np.random.randint(-250,150,1000)\n",
    "X[:,3] = 10*np.random.randn(1000)-75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ee18b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -320.2545309 ,    -9.3904832 ,   -85.        ,   -89.85286099],\n",
       "       [ 1090.50674007,    -9.01642287,    60.        ,   -84.00231713],\n",
       "       [ 1528.86149075,    -9.4104554 ,  -109.        ,   -78.22931661],\n",
       "       ...,\n",
       "       [-1334.77532715,    -9.56466439,  -142.        ,   -62.87871873],\n",
       "       [ -699.54356749,    -9.97809278,    85.        ,   -74.17383445],\n",
       "       [ 1722.04293527,    -9.12892911,   -94.        ,   -74.72262628]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4234a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of X1: 11.935201655200657\n",
      "variance of X1: 1026903.0991374647\n",
      "\n",
      "mean of X2: -9.503242684051795\n",
      "variance of X2: 0.0821580848191559\n",
      "\n",
      "mean of X3: -52.442\n",
      "variance of X3: 13772.400636\n",
      "\n",
      "mean of X4: -74.45477168608731\n",
      "variance of X4: 96.85199591904106\n"
     ]
    }
   ],
   "source": [
    "## demonstrating the different scales\n",
    "print(\"mean of X1:\",np.mean(X[:,0]))\n",
    "print(\"variance of X1:\",np.var(X[:,0]))\n",
    "print()\n",
    "\n",
    "print(\"mean of X2:\",np.mean(X[:,1]))\n",
    "print(\"variance of X2:\",np.var(X[:,1]))\n",
    "print()\n",
    "\n",
    "print(\"mean of X3:\",np.mean(X[:,2]))\n",
    "print(\"variance of X3:\",np.var(X[:,2]))\n",
    "print()\n",
    "\n",
    "print(\"mean of X4:\",np.mean(X[:,3]))\n",
    "print(\"variance of X4:\",np.var(X[:,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54830fd4",
   "metadata": {},
   "source": [
    "We can see that the columns of `X` have very different scales. We will soon learn some algorithms whose results can be greatly distorted when this happens.\n",
    "\n",
    "The main approach to fixing this issue is to scale the data so they are all on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d97f9",
   "metadata": {},
   "source": [
    "## Standardizing your data\n",
    "\n",
    "While there are a few different ways to scale data, one of the most common is to <i>standardize</i> it. When you standardize a variable, $x$, you apply the following transformation:\n",
    "\n",
    "$$\n",
    "x_\\text{scaled} = \\frac{x - \\text{mean}(x)}{\\text{standard deviation}(x)},\n",
    "$$\n",
    "\n",
    "if you have taken a statistics course (or used $Z$-tables), this should look familiar. This is precisely the transformation applied to turn any arbitary normal random variable into a <i>standard normal</i> random variable, hence the term <i>standardizing</i>.\n",
    "\n",
    "Standardizing your data will transform it to have mean $0$ and standard deviation $1$.\n",
    "\n",
    "### `StandardScaler`\n",
    "\n",
    "We could do this by hand using `numpy`, but that will quickly become tedious. `sklearn` provides a nice `scaler` object called `StandardScaler` that will perform this on all columns of your data set, and has functionality that plays nicely with train test splits. Here is the documentation <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51f36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bbf42b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "## fit the scaler\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b518c8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.93520166,  -9.50324268, -52.442     , -74.45477169])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d8b183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01336227e+03, 2.86632316e-01, 1.17355872e+02, 9.84134116e+00])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc7846c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -320.2545309 ,    -9.3904832 ,   -85.        ,   -89.85286099],\n",
       "       [ 1090.50674007,    -9.01642287,    60.        ,   -84.00231713],\n",
       "       [ 1528.86149075,    -9.4104554 ,  -109.        ,   -78.22931661],\n",
       "       ...,\n",
       "       [-1334.77532715,    -9.56466439,  -142.        ,   -62.87871873],\n",
       "       [ -699.54356749,    -9.97809278,    85.        ,   -74.17383445],\n",
       "       [ 1722.04293527,    -9.12892911,   -94.        ,   -74.72262628]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e690b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale the data, i.e. transform it\n",
    "X_scale = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db90ab98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.07910092e-02,  2.61772391e+01,  1.13707135e+00,\n",
       "         8.58162219e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(np.array([[1, -2, 81, 9.9999]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d085ba66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of standardized X1: -2.4868995751603507e-17\n",
      "variance of standardized X1: 1.000000000000001\n",
      "\n",
      "mean of standardized X2: -3.849809360190193e-14\n",
      "variance of standardized X2: 0.9999999999999999\n",
      "\n",
      "mean of standardized X3: 4.085620730620576e-17\n",
      "variance of standardized X3: 0.9999999999999991\n",
      "\n",
      "mean of standardized X4: -1.0054179711005418e-14\n",
      "variance of standardized X4: 1.0\n"
     ]
    }
   ],
   "source": [
    "## Checking the scaled means and variances\n",
    "print(\"mean of standardized X1:\",np.mean(X_scale[:,0]))\n",
    "print(\"variance of standardized X1:\",np.var(X_scale[:,0]))\n",
    "print()\n",
    "\n",
    "print(\"mean of standardized X2:\",np.mean(X_scale[:,1]))\n",
    "print(\"variance of standardized X2:\",np.var(X_scale[:,1]))\n",
    "print()\n",
    "\n",
    "print(\"mean of standardized X3:\",np.mean(X_scale[:,2]))\n",
    "print(\"variance of standardized X3:\",np.var(X_scale[:,2]))\n",
    "print()\n",
    "\n",
    "print(\"mean of standardized X4:\",np.mean(X_scale[:,3]))\n",
    "print(\"variance of standardized X4:\",np.var(X_scale[:,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d58851b",
   "metadata": {},
   "source": [
    "#### `fit`, `transform` and `fit_transform` & scaling for train test splits\n",
    "\n",
    "You may be wondering what `fit`, `transform` and `fit_transform` do. Let's describe:\n",
    "- `fit` performs a fit of the `scaler` object, for `StandardScaler` this means finding the mean and standard deviation of each column and storing it. `fit` must be called <i>before</i> `transform`.\n",
    "- `transform` is what actually performs the scaling, for `StandardScaler` this means substracting the respective means and dividing by the respective standard deviations for each column. `transform` must be called <i>after</i> `fit`.\n",
    "- `fit_transform` does this all in one fell swoop, i.e. it fits the `scaler` object then uses the fit to transform the data.\n",
    "\n",
    "##### Why do we need anything other than `fit_transform`?\n",
    "\n",
    "Excellent question! In the example above we probably could have just used `fit_transform`.\n",
    "\n",
    "However, this is because we were not dealing with train test splits, validation sets or cross-validation. We consider scaling the data part of the model, meaning the algorithm/model was fit using the data scaled with the training set's means and standard deviations. For example, if we scaled data prior to fitting a linear regression model, then $\\hat{\\beta}$ was found using the data scaled according to the training set (using the means and standard deviations of the training set columns). To assess how that particular model performs we must also scale any validation set, cross-validation holdout set or test set using the same exact scaling (i.e. using the means and standard deviations from the training set).\n",
    "\n",
    "Let's illustrate what I mean with a quick final example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b82d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e28607de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X,\n",
    "                                      shuffle=True,\n",
    "                                      random_state=614,\n",
    "                                      test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c55b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit then transform the training set\n",
    "scaler_new = StandardScaler()\n",
    "\n",
    "scaler_new.fit(X_train)\n",
    "\n",
    "X_train_scale = scaler_new.transform(X_train)\n",
    "\n",
    "## alternatively I could do\n",
    "## X_train_scale = scaler_new.fit_transform(X_train), Why?\n",
    "## because this is the training set, so it is okay to run fit_transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ea833",
   "metadata": {},
   "source": [
    "<i>Imagine we build a model here.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccc04517",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the test set\n",
    "## DO NOT refit the scaler!\n",
    "X_test_scale = scaler_new.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb112b",
   "metadata": {},
   "source": [
    "<i>Imagine we test model performance on the test set here.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f19398a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f58b21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(5, shuffle=True, random_state = 203)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "394d596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 22.52538957  -9.50562737 -49.0875     -74.45829967]\n",
      "\n",
      "[ 18.07635978  -9.50923032 -53.121875   -74.44154339]\n",
      "\n",
      "[ 26.38859272  -9.4999481  -46.5734375  -74.18682472]\n",
      "\n",
      "[ 50.96297419  -9.49825098 -54.6828125  -74.76305543]\n",
      "\n",
      "[ 62.77142348  -9.50599823 -53.228125   -74.45668387]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kfold.split(X_train):\n",
    "    X_tt = X_train[train_index, :]\n",
    "    X_ho = X_train[test_index, :]\n",
    "    \n",
    "    \n",
    "    ## make a new scaler here\n",
    "    scaler_kfold = StandardScaler()\n",
    "    \n",
    "    scaler_kfold.fit(X_tt)\n",
    "    \n",
    "    print(scaler_kfold.mean_)\n",
    "    print()\n",
    "    \n",
    "    X_tt_scaled = scaler_kfold.transform(X_tt)\n",
    "    \n",
    "    X_ho_scaled = scaler_kfold.transform(X_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46040e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0fc9250",
   "metadata": {},
   "source": [
    "## Other scaler objects\n",
    "\n",
    "`sklearn` has more scalers than just `StandardScaler`, which you can find at this link, <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\">https://scikit-learn.org/stable/modules/preprocessing.html</a>. Moreover, some `sklearn` models have their own input arguments that will handle scaling for you when set `= True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86570f4",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32263ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
