{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9b68bb",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "\n",
    "Sometimes our features may be missing observations, what can we do.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss a method to deal with missing values,\n",
    "- Demonstrate that method on a penguin data set,\n",
    "- Illustrate various approaches to imputation and\n",
    "- Show how to integrate imputation into a train test split procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ffc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seaborn import set_style\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a905250",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "There will be times when you need to work with a data set that has missing data. We can see one such example with this edited `seaborn` penguins data set, <a href=\"https://seaborn.pydata.org/examples/scatterplot_matrix.html\">https://seaborn.pydata.org/examples/scatterplot_matrix.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738558c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"../../Data/penguins_w_nas.csv\")\n",
    "\n",
    "penguins.loc[penguins.sex=='Male', 'sex'] = 0\n",
    "penguins.loc[penguins.sex=='Female', 'sex'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80868262",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae74d2",
   "metadata": {},
   "source": [
    "Using `.info()` above we can see that the data set has 344 entries with some observations that are missing values for one or more columns. If there are NAs in features that you plan to use in a model you will be unable to use those observations in training or validation sets.\n",
    "\n",
    "So what can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27bf175",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "The process of replacing missing values in data is known as <i>imputation</i>. There are a few different ways you can impute missing data.\n",
    "\n",
    "### Imputing a preset constant value\n",
    "\n",
    "The simplest approach is just just impute a constant value. For example, maybe scientists have already observed an average body mass for penguins, let's say it is `4207`. You could then impute this value for all missing `body_mass_g` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92faf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a copy to demonstrate impute strategy\n",
    "penguins_constant_impute = penguins.copy()\n",
    "\n",
    "## Replace the missing data\n",
    "## .isna() checks for missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_constant_impute.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8075466",
   "metadata": {},
   "source": [
    "### Imputing based on a sample statistic\n",
    "\n",
    "A common strategy is to impute missing values using sample statistics from the non-missing values in a column. For example, we can replace NAs with the mean, median or mode of the column.\n",
    "\n",
    "While we can do this by hand using `numpy` or `pandas` it may be easier, and more easily used in predictive modeling, to use `sklearn`'s `SimpleImputer`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\">https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ce1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429015c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a SimpleImputer object\n",
    "## strategy determines how the imputation happens\n",
    "## options are 'mean', 'median', 'most_frequent' and 'constant'\n",
    "impute = \n",
    "\n",
    "## fit the impute object\n",
    "\n",
    "\n",
    "## Show the transformed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf74227",
   "metadata": {},
   "source": [
    "### Building a model to impute\n",
    "\n",
    "We can also build a model to impute the missing values. Let's regress `body_mass_g` on all other columns to get imputed values for the missing data.\n",
    "\n",
    "Note that we cannot do this for observations missing multiple features. For now we will ignore those rows, but in practice we would have to come up for unique imputation strategies for each columns (and sometimes each row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd46452",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_na = penguins.loc[penguins.body_mass_g.isna()].dropna(subset=['bill_length_mm', \n",
    "                                                                       'bill_depth_mm', \n",
    "                                                                       'flipper_length_mm', \n",
    "                                                                       'sex']).copy()\n",
    "penguins_non_na = penguins.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_na[['Adelie', 'Gentoo']] = pd.get_dummies(penguins_na.species)[['Adelie', 'Gentoo']].copy()\n",
    "penguins_non_na[['Adelie', 'Gentoo']] = pd.get_dummies(penguins_non_na.species)[['Adelie', 'Gentoo']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(copy_X=True)\n",
    "\n",
    "reg.fit(penguins_non_na[['bill_length_mm', \n",
    "                         'bill_depth_mm', \n",
    "                         'flipper_length_mm', \n",
    "                         'sex', \n",
    "                         'Adelie', \n",
    "                         'Gentoo']].values,\n",
    "        penguins_non_na.body_mass_g.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(penguins_na[['bill_length_mm', \n",
    "                         'bill_depth_mm', \n",
    "                         'flipper_length_mm', \n",
    "                         'sex', \n",
    "                         'Adelie', \n",
    "                         'Gentoo']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9d43f",
   "metadata": {},
   "source": [
    "For missing values that occur in rows where modeling is impossible you would have to use one of the other prior strategies.\n",
    "\n",
    "#### A note about predictive modeling\n",
    "\n",
    "If the end goal for your data set is to build a predictive model you <b>cannot</b> use the column that you are trying to predict in an imputation model. For example, if we were looking to build a model that predicted the `species` of the penguin we would not be allowed to use it for feature imputation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9554cf",
   "metadata": {},
   "source": [
    "## Imputation in predictive modeling projects\n",
    "\n",
    "Recall that when we scale data in predictive modeling projects we have to fit the scaler on the training data and then use that fit scaler on the test data. Importantly, we do <b>not</b> refit the scaler on the test or holdout data. We have to take a similar approach to any imputation technique.\n",
    "\n",
    "For example:\n",
    "- Imputation using any sample statistic approach must use the sample statistic computed on the training data to impute missing values in the test, validation or holdout sets and\n",
    "- Models trained to impute missing values must be fit on the training data and that fitted model is what used on the test data.\n",
    "\n",
    "`sklearn`'s `SimpleImputer` object is nicely set up for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b653f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14447d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_train, penguins_test = train_test_split(penguins, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=32,\n",
    "                                                    test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17867a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define an imputer with the 'mean' strategy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## fitting the imputer on the training set\n",
    "impute.fit(penguins_train[['bill_length_mm', \n",
    "                          'bill_depth_mm', \n",
    "                          'flipper_length_mm',\n",
    "                          'body_mass_g']])\n",
    "\n",
    "## Imputing the training set\n",
    "impute.transform(penguins_train[['bill_length_mm', \n",
    "                              'bill_depth_mm', \n",
    "                              'flipper_length_mm',\n",
    "                              'body_mass_g']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6e389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Imputing the test set\n",
    "impute.transform(penguins_test[['bill_length_mm', \n",
    "                          'bill_depth_mm', \n",
    "                          'flipper_length_mm',\n",
    "                          'body_mass_g']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94f2ca",
   "metadata": {},
   "source": [
    "`SimpleImputer` (and `sklearn`'s other imputer objects, <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute\">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute</a>) can be implemented into a pipeline just like `StandardScaler` or any other `sklearn` preprocessing object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75f670",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc388608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
