{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$ Means Clustering\n",
    "\n",
    "Our first clustering method sounds like $k$ nearest neighbors, but has nothing to do with it.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Lay out the idea of the $k$ means clustering algorithm,\n",
    "- Demonstrate it on some phony data and\n",
    "- Discuss different ways you can choose the algorithm's $k$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have $n$ observations of $m$ features, $X$, that we suspect could be reasonably segmented into groups. The $k$ means algorithm gives us a way to attempt and find those groups in our data.\n",
    "\n",
    "### The algorithm\n",
    "\n",
    "In $k$ means clustering we:\n",
    "\n",
    "1. Select $k$ initial points as a first guess for the $k$ groups' <i>centroids</i>, which are defined to be the average position of all the points within a group,\n",
    "2. Group all $n$ points according to which centroid is closest,\n",
    "3. Recalculate the $k$ centroids using the groups found in step 2 and\n",
    "4. Repeat steps $2$ and $3$ until you reach a time at which no observations change groups.\n",
    "\n",
    "#### By hand\n",
    "\n",
    "Let's see this in action, then we will demonstrate how to implement it in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X = np.zeros((30,2))\n",
    "\n",
    "X[:10,:] = np.random.randn(10,2)\n",
    "X[10:20,:] = np.random.randn(10,2) + np.array([4,4])\n",
    "X[20:,:] = np.random.randn(10,2) + np.array([-5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random choice of centroid\n",
    "centroid_guess = np.zeros((3,2))\n",
    "\n",
    "centroid_guess[:,0] = 12*np.random.random(3) - 7\n",
    "centroid_guess[:,1] = 8*np.random.random(3) - 1\n",
    "\n",
    "## find distance from each point to each centroid\n",
    "distances = np.zeros((30,3))\n",
    "\n",
    "distances[:,0] = np.sqrt(np.sum(np.power(X - centroid_guess[0,:],2), axis=1))\n",
    "distances[:,1] = np.sqrt(np.sum(np.power(X - centroid_guess[1,:],2), axis=1))\n",
    "distances[:,2] = np.sqrt(np.sum(np.power(X - centroid_guess[2,:],2), axis=1))\n",
    "\n",
    "clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "## recalculate cluster centroids\n",
    "new_centroid_guess = np.zeros((3,2))\n",
    "new_centroid_guess[0,:] = np.mean(X[clusters==0,:], axis=0)\n",
    "new_centroid_guess[1,:] = np.mean(X[clusters==1,:], axis=0)\n",
    "new_centroid_guess[2,:] = np.mean(X[clusters==2,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(X[clusters==0,0], X[clusters==0,1], c='b', label=\"Current Cluster 0\")\n",
    "plt.scatter(X[clusters==1,0], X[clusters==1,1], c='green', marker='v', label=\"Current Cluster 1\")\n",
    "plt.scatter(X[clusters==2,0], X[clusters==2,1], c='k', marker='+', s=100, label=\"Current Cluster 2\")\n",
    "\n",
    "plt.scatter(centroid_guess[:,0], \n",
    "            centroid_guess[:,1], \n",
    "            c='r', \n",
    "            marker='x', s=100, label='Current Centroid Guess')\n",
    "plt.scatter(new_centroid_guess[:,0], new_centroid_guess[:,1], c='orange', marker='*', s=100, label='Next Centroid Guess')\n",
    "\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"First Round of $k$-Means\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_guess = new_centroid_guess.copy()\n",
    "\n",
    "## find distance from each point to each centroid\n",
    "distances[:,0] = np.sqrt(np.sum(np.power(X - centroid_guess[0,:],2), axis=1))\n",
    "distances[:,1] = np.sqrt(np.sum(np.power(X - centroid_guess[1,:],2), axis=1))\n",
    "distances[:,2] = np.sqrt(np.sum(np.power(X - centroid_guess[2,:],2), axis=1))\n",
    "\n",
    "clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "## recalculate cluster centroids\n",
    "new_centroid_guess = np.zeros((3,2))\n",
    "new_centroid_guess[0,:] = np.mean(X[clusters==0,:], axis=0)\n",
    "new_centroid_guess[1,:] = np.mean(X[clusters==1,:], axis=0)\n",
    "new_centroid_guess[2,:] = np.mean(X[clusters==2,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(X[clusters==0,0], X[clusters==0,1], c='b', label=\"Current Cluster 0\")\n",
    "plt.scatter(X[clusters==1,0], X[clusters==1,1], c='green', marker='v', label=\"Current Cluster 1\")\n",
    "plt.scatter(X[clusters==2,0], X[clusters==2,1], c='k', marker='+', s=100, label=\"Current Cluster 2\")\n",
    "\n",
    "plt.scatter(centroid_guess[:,0], centroid_guess[:,1], c='r', marker='x', s=100, label='Current Centroid Guess')\n",
    "plt.scatter(new_centroid_guess[:,0], new_centroid_guess[:,1], c='orange', marker='*', s=100, label='Next Centroid Guess')\n",
    "\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Second Round of $k$-Means\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would keep going in this way until our stopping criterion was hit. For this example it seems like that could be this very round.\n",
    "\n",
    "#### In `sklearn`\n",
    "\n",
    "We can implement this in `sklearn` with `KMeans`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a KMeans object\n",
    "kmeans = \n",
    "\n",
    "## Fit the kmeans object\n",
    "\n",
    "\n",
    "\n",
    "## get the clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(X[clusters==0,0], X[clusters==0,1], c='b', label=\"$k$-Means Cluster 0\")\n",
    "plt.scatter(X[clusters==1,0], X[clusters==1,1], c='green', marker='v', label=\"$k$-Means Cluster 1\")\n",
    "plt.scatter(X[clusters==2,0], X[clusters==2,1], c='k', marker='+', s=100, label=\"$k$-Means Cluster 2\")\n",
    "\n",
    "## You can get the centers with cluster_centers_\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], \n",
    "            kmeans.cluster_centers_[:,1], \n",
    "            c='r', marker='x', s=100, label='$k$-Means Centroid')\n",
    "\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"$k$-Means Clustering\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose $k$?\n",
    "\n",
    "While we knew to choose $k=3$ for this problem, it is not clear, in general, what the best value of $k$ is. Typically you will have to run the algorithm multiple times with different values of $k$ and examine some metrics to determine which value of $k$ was \"best\".\n",
    "\n",
    "We will now present two methods. First we will describe the method, then we will demonstrate it on a new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt(\"../../../Data/kmeans.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The elbow method\n",
    "\n",
    "The first approach we can take to determine a good value of $k$ is known as the <i>elbow method</i>. In the elbow method we calculate the <i>inertia</i> of the resulting clustering for each value of $k$ and then look for an <i>elbow</i> in the plot of inerita against $k$.\n",
    "\n",
    "##### Inertia\n",
    "\n",
    "For a given clustering with $k$ clusters we define the inertia to be:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\text{dist}(X^{(i)}, c^{(i)})^2,\n",
    "$$\n",
    "\n",
    "where $X^{(i)}$ is the $i^\\text{th}$ observation in the data set, $c^{(i)}$ is the centroid of the cluster to which observation $i$ is assigned and $\\text{dist}(a,b)$ denotes the distance between points $a$ and $b$.\n",
    "\n",
    "We think of a clustering with low inertia as being good, with the caveat that we cannot just choose the value of $k$ that gives the lowest inertia. We could arbitrarily get an inertia of $0$ by settng $k=n$.\n",
    "\n",
    "The elbow method plots inertia against $k$ and looks for an \"elbow\" in the plot, which is indicative of diminishing returns from increasing $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a loop to record inertias\n",
    "inertias = []\n",
    "\n",
    "for k in range(1,11):\n",
    "    kmeans = \n",
    "    \n",
    "    ## you can get the inertia from a fit KMeans object\n",
    "    ## with .inertia_\n",
    "    inertias.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(1,11), inertias, '-o')\n",
    "\n",
    "plt.xlabel(\"$k$\", fontsize=18)\n",
    "plt.ylabel(\"Inertia\", fontsize=18)\n",
    "\n",
    "plt.xticks(range(1,11),fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we could reasonably say that the \"elbow\" occurs at $4$ or $5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The silhouette method\n",
    "\n",
    "A second method for determining a good number of clusters is known as the <i>silhouette method</i> which involves calculating the <i>silhouette score</i> for a given clustering.\n",
    "\n",
    "#### Silhouette score\n",
    "\n",
    "For a given observation $i$ the silhouette score for $i$ is defined to be:\n",
    "\n",
    "$$\n",
    "\\frac{b-a}{\\max(a,b)},\n",
    "$$\n",
    "\n",
    "where $a$ is the average distance of observation $i$ to all other points in its assigned cluster, and $b$ is the average distance of observation $i$ to all the observations in the next closest cluster to observation $i$.\n",
    "\n",
    "For a given clustering, we would take the average silhouette score over all $n$ observations.\n",
    "\n",
    "In `sklearn` this can be found with `silhouette_score`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html</a>, for the cluster score and `silhouette_samples` for the individual observations <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html</a>.\n",
    "\n",
    "For this metric a higher score is some indication of a \"good\" clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import from metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a sample clustering\n",
    "kmeans = KMeans(4).fit(X)\n",
    "clusters = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show cluster wide score\n",
    "## call silhouette_score with the features first\n",
    "## then the clusters second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## show individual scores with silhouette_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a loop to record inertias\n",
    "sils = []\n",
    "\n",
    "for k in range(2,11):\n",
    "    kmeans = KMeans(k).fit(X)\n",
    "    \n",
    "    ## you can get the inertia from a fit KMeans object\n",
    "    ## with .inertia_\n",
    "    sils.append(silhouette_score(X, kmeans.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(2,11), sils, '-o')\n",
    "\n",
    "plt.xlabel(\"$k$\", fontsize=18)\n",
    "plt.ylabel(\"Silhouette Score\", fontsize=18)\n",
    "\n",
    "plt.xticks(range(2,11),fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be helpful when selecting $k$ to look at what is known as the <i>silhouette diagram</i>, which plots the distribution of silhouette scores for each cluster along with the sample average score. Our goal with such plots is to avoid clusters with distributions completely to the left of the average score line (red dotted line below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatch_dict={0:'',\n",
    "               1:'/',\n",
    "               2:'*',\n",
    "               3:'+',\n",
    "               4:'\\\\',\n",
    "               5:'o'}\n",
    "\n",
    "fig,ax = plt.subplots(2, 2, figsize=(18,15))\n",
    "\n",
    "i = 0\n",
    "for k in [3,4,5,6]:\n",
    "    kmeans = KMeans(k).fit(X)\n",
    "    clusters = kmeans.predict(X)\n",
    "    \n",
    "    y_lower = 0\n",
    "\n",
    "    cluster_ticks = []\n",
    "    \n",
    "    for c in range(k):\n",
    "        c_scores = silhouette_samples(X, clusters)[clusters==c]\n",
    "        c_scores.sort()\n",
    "\n",
    "\n",
    "        ax[i//2,i%2].fill_betweenx(\n",
    "                            np.arange(y_lower, y_lower + np.sum(clusters==c)),\n",
    "                            0,\n",
    "                            c_scores,\n",
    "                            facecolor= plt.cm.tab10(float(c) / k),\n",
    "                            edgecolor='black',\n",
    "                            hatch=hatch_dict[c],\n",
    "                            alpha=0.5,\n",
    "                        )\n",
    "        cluster_ticks.append((y_lower + y_lower + np.sum(clusters==c))/2)\n",
    "        y_lower = y_lower + np.sum(clusters==c)\n",
    "\n",
    "    ax[i//2,i%2].plot([silhouette_score(X, clusters), silhouette_score(X, clusters)],[0,y_lower], 'r--')\n",
    "    ax[i//2,i%2].set_title(\"$k=$\"+str(k), fontsize=18)\n",
    "    ax[i//2,i%2].set_yticks(cluster_ticks)\n",
    "    ax[i//2,i%2].set_yticklabels(range(1,k+1), fontsize=16)\n",
    "    ax[i//2,i%2].tick_params(axis='x',labelsize=16)\n",
    "    i = i + 1\n",
    "\n",
    "ax[0,0].set_ylabel(\"Cluster\", fontsize=18)\n",
    "ax[1,0].set_ylabel(\"Cluster\", fontsize=18)\n",
    "ax[1,0].set_xlabel(\"Silhouette Score\", fontsize=18)\n",
    "ax[1,1].set_xlabel(\"Silhouette Score\", fontsize=18)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fromm the above plots we can see that $k=3$ and $k=5$ have all \"good\" clusters, but $k=3$'s $2^\\text{nd}$ cluster has the vast majority of its points to the left of the average line.\n",
    "\n",
    "Because $k=5$ was:\n",
    "- a potential elbow case,\n",
    "- has the highest silhouette score and\n",
    "- has no \"bad\" clusters according to our silhouette diagram\n",
    "\n",
    "we will choose $k=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "$k$ means has been shown to be a fast and scalable clustering algorithm that is easy to implement. However, there are some limitations:\n",
    "- Random initialization of centroids can lead to suboptimal solutions:\n",
    "    - see the associated `Practice Problems` notebook for some comments on this,\n",
    "- Choosing $k$ can be tricky:\n",
    "    - The elbow method is a bit coarse and\n",
    "    - The silhouette method can be computationally expensive,\n",
    "- Can be ill-behaved when:\n",
    "    - Clusters have varying sizes,\n",
    "    - Clusters have different densities or nonspherical shapes and\n",
    "    - Features are on different scales:\n",
    "        - Always scale your data prior to fitting `KMeans`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
