{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "A second clustering algorithm.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the idea hierarchical clustering:\n",
    "    - Learn about dendrograms and cut points and\n",
    "- Show how to implement it with `scipy`:\n",
    "    - Learn how to plot a dendrogram in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is another technique for grouping $n$ observations of $m$ features stored in a matrix, $X$. A nice feature of hierarchical clustering, in comparison to $k$ means, is that we <i>do not</i> need to guess a number of clusters prior to fitting the algorithm. Instead we examine something known as a <i>dendrogram</i> and use that to make an informed choice after running the algorithm.\n",
    "\n",
    "## The algorithm\n",
    "\n",
    "In hierarchical clustering you start from each observation being its own cluster and slowly work your way to having every observation in a single cluster. This is done by combining clusters according to an inter-cluster distance measure that you determine prior to fitting the algorithm, let's call this $d$.\n",
    "\n",
    "Starting from $d=0$ you slowly increase $d$ and when any pair of clusters are $d$ apart from one another you combine them. Let's demonstrate this with a few sketches.\n",
    "\n",
    "<img src=\"hclust1.png\" width = \"50%\"></img>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"hclust2.png\" width = \"50%\"></img>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"hclust3.png\" width = \"50%\"></img>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"hclust4.png\" width = \"60%\"></img>\n",
    "\n",
    "This clustering information is then stored in a dendrogram:\n",
    "\n",
    "<img src=\"dendro.png\" width = \"35%\"></img>\n",
    "\n",
    "A variety of clusterings can then be selected depending on which distance you choose to make a cut point.\n",
    "\n",
    "## In `scipy`\n",
    "\n",
    "Rather than `sklearn` we will use `scipy` to perform hierarchical clustering, <a href=\"https://docs.scipy.org/\">https://docs.scipy.org/</a>. \n",
    "\n",
    "Let's make a data set to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(323)\n",
    "X = np.zeros((20,2))\n",
    "\n",
    "X[:10,:] = np.random.randn(10,2)\n",
    "X[10:15,:] = np.random.randn(5,2) + np.array([4,4])\n",
    "X[15:,:] = np.random.randn(5,2) + np.array([-5,4])\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy` keeps its hierarchical clustering functions/objects in the `cluster.hierarchy` module. In particular we will want `dendrogram`, <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html</a> and `linkage` <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we run linkage\n",
    "## Put in the data, followed by the method\n",
    "## we use method = 'single'\n",
    "## this uses the minimum distance between points in clusters A and B\n",
    "\n",
    "\n",
    "## this returns a numpy array that we will now describe\n",
    "pd.DataFrame(Z, columns = ['cluster_1', 'cluster_2', 'distance', 'new_cluster_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`linkage` returns an array that tracks everytime that two clusters merge, this is known as a linkage matrix.\n",
    "\n",
    "<i>Note:</i> we can control how `linkage` measures the distance between clusters with the `method` argument. The default is `method='single'` which finds the minimum distance between any pair of points between two clusters. Other options include:\n",
    "- `method='complete'`, which is the maximum distance between any pair of points between two clusters,\n",
    "- `method='average'`, which is the average distance between any pair of points between two cluster,\n",
    "- `method='centroid'`, which is the distance between the centroids of two clusters and\n",
    "- `method='ward'`, which uses the Ward variance minimization algorithm.\n",
    "\n",
    "Once we have the linkage matrix we can run `dendrogram` to plot the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "## This plots a dendrogram from the linkage matrix\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the colors above seem like a reasonable clustering, it is important to note that this is a coincidence. In `dendrogram` there is an argument called `color_threshold` which colors the clusters according to the clustering at that cut point. The default value for this is `color_threshold = 0.7*max(Z[:,2])`, which is $0.7$ times the maximum merge distance in the linkage matrix. We demonstrate this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The default color_threshold\n",
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "dendrogram(Z)\n",
    "\n",
    "plt.plot([0,200], \n",
    "         [0.7*max(Z[:,2]), 0.7*max(Z[:,2])], \n",
    "         'k--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A lower color_threshold\n",
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "## What do we get with a different color_threshold .4*max(Z[:,2])?\n",
    "dendrogram(Z, )\n",
    "\n",
    "plt.plot([0,200], \n",
    "         [0.4*max(Z[:,2]), 0.4*max(Z[:,2])], \n",
    "         'k--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a cutpoint you like, you need to actually get the cluster for each point. We do this with `fcluster`, <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the clusters\n",
    "## First put the linkage matrix\n",
    "## then the cutoff point,\n",
    "## then the criterion\n",
    "## we use 'distance', which just uses the distance between points illustrated\n",
    "## in the dendrograms above\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our corresponding clustering\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(X[clusters == 1,0],\n",
    "            X[clusters == 1,1],\n",
    "            label=\"Cluster 1\")\n",
    "\n",
    "plt.scatter(X[clusters == 2,0],\n",
    "            X[clusters == 2,1],\n",
    "            marker = 'v',\n",
    "            s = 50,\n",
    "            label=\"Cluster 2\")\n",
    "\n",
    "plt.scatter(X[clusters == 3,0],\n",
    "            X[clusters == 3,1],\n",
    "            marker = '+',\n",
    "            s = 100,\n",
    "            label=\"Cluster 3\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to $k$ means clustering we could use an inertia elbow plot or silhouette plots to help us determine what to set as our threshold value, but sometimes it is reasonable to just use a visual inspection. Looking at this particular dendrogram a threshold that produces three clusters looks reasonable.\n",
    "\n",
    "Further, there may be other consideration that come from the particular problem on which you are working. For example, maybe you are working on a market segmentation problem for a business and additional market segments (clusters) can increase profit while also increasing cost. You would likely need to consider those which are external to whether the clustering is the \"best\" segmentation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
