{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704aaa59",
   "metadata": {},
   "source": [
    "# Principal Components Analysis (PCA)\n",
    "\n",
    "One of the most popular dimension reduction technique in data science involves projecting the data down onto orthogonal variance-preserving directions.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Learn the concept behind PCA,\n",
    "- Review the mathematics underlying PCA,\n",
    "- Show how to implement PCA in `sklearn`,\n",
    "- Demonstrate the explained variance and the explained variance curve:\n",
    "    - Introduce the labeled faces in the wild data set and\n",
    "- See how we can attempt to interpret the results of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5778fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a dark background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b26ef0",
   "metadata": {},
   "source": [
    "## The intuition behind PCA\n",
    "\n",
    "When you reduce the dimension of a data set you are losing information, so you want to ensure that you reduce in a way that \"retains as much information as possible\". PCA tackles this problem in a very statistical manner.\n",
    "\n",
    "There is an idea in statistics that the information of a data set is located within that data set's variance. PCA thus looks to reduce the dimension of a data set by projecting the data  onto a lower dimensional space that captures as much of the original variance as possible. Thinking in terms of optimization, your goal is to project into a lower dimensional space in a way that maximizes variance.\n",
    "\n",
    "Here is a heuristic algorithm:\n",
    "1. Center your data so that each feature has 0 mean, this is done for convenience.\n",
    "2. Find the direction in space along which projections have the highest variance, this produces the first principal component.\n",
    "3. Find the direction orthogonal to the first principal component that maximizes variance, this is the second principal component.\n",
    "4. Continue in this way, the kth principal component is the variance-maximizing direction orthogonal to the previous k-1 components.\n",
    "\n",
    "Let's see what we mean in a 2-D example, we will use `sklearn`'s `PCA` object, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some random data\n",
    "np.random.seed(440)\n",
    "\n",
    "x1 = 9*np.random.randn(500)\n",
    "x2 = 2*np.random.randn(500)\n",
    "\n",
    "X = np.concatenate([x1.reshape(-1,1),x2.reshape(-1,1)], axis = 1)\n",
    "\n",
    "angle = -np.pi/4\n",
    "\n",
    "X = X.dot(np.array([[np.cos(angle),-np.sin(angle)],[np.sin(angle),np.cos(angle)]]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(\"$x_2$\", fontsize=14)\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d957ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA is stored in decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the PCA object\n",
    "## we'll project down to 2-D\n",
    "\n",
    "\n",
    "## Fit the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will draw a vector in 2D\n",
    "## given its components\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, \n",
    "                    shrinkB=0,\n",
    "                    color=\"black\")\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=.4)\n",
    "\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(\"$x_2$\", fontsize=14)\n",
    "\n",
    "plt.title(\"Original Data and Component Vectors\", fontsize=18)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594cdcc4",
   "metadata": {},
   "source": [
    "The vectors drawn above are called the <i>component vectors</i> of the PCA. When we want to get the transformed version of the data, we get the scalar projection of each observation onto the component vectors. We'll visualize this more explicitly after reviewing the math behind PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform gets you the PCA transformed values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(fit[:,0], fit[:,1], alpha=.8)\n",
    "\n",
    "plt.xlabel(\"First PCA value\", fontsize=16)\n",
    "plt.ylabel(\"Second PCA value\", fontsize=16)\n",
    "\n",
    "plt.title(\"PCA Transformed Data\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ab38c",
   "metadata": {},
   "source": [
    "## The math behind PCA\n",
    "\n",
    "We consider the maximal variance formulation of the problem, for those interested in equivalent formulations check the associated `Practice Problems` notebook.\n",
    "\n",
    "Suppose we have $n$ observations of $m$ features, let $X_1, X_2, \\dots, X_m$ be $n$ by $1$ vectors containing the observations of each of the $m$ features. And for ease of notation assume each has been centered to have mean $0$.\n",
    "\n",
    "We restrict ourselves to the case of finding the first principal component, the others can be found in a similar fashion.\n",
    "\n",
    "Let \n",
    "$$\n",
    "X = \\left(X_1 | X_2 | \\dots |X_m \\right)\n",
    "$$\n",
    "be an $n$ by $m$ feature matrix.\n",
    "\n",
    "Our goal is to find $w=(w_1,w_2,\\dots,w_m)^T$ with $||w|| = 1$, such that $\\text{Var}(w_1 X_1 + w_2 X_2 + \\dots + w_m X_m) = \\text{Var}( X w)$ is maximized (note that because $||w||=1$, $Xw$ is a vector of scalar projections of the rows of $X$ onto $w$).\n",
    "\n",
    "Because we have centered the columns of $X$ we have:\n",
    "\n",
    "$$\n",
    "\\text{Var}(Xw) = E(w^T X^T X w) = w^T E(X^T X) w = w^T \\Sigma w,\n",
    "$$\n",
    "\n",
    "where $\\Sigma$ is the covariance matrix of $X$.\n",
    "\n",
    "Our constrained optimization problem is now:\n",
    "\n",
    "$$\n",
    "\\text{optimize } f(w) = w^T \\Sigma w, \\text{ constrained to } g(w) = w^T w - 1 = 0.\n",
    "$$\n",
    "\n",
    "Using the method of Lagrange multipliers and some matrix calculus (see the references below for a helpful matrix calculus resource):\n",
    "$$\n",
    "\\partial_w \\left(w^T \\Sigma w - \\lambda (w^T w - 1)\\right) = 2 \\Sigma w - 2\\lambda w.\n",
    "$$\n",
    "\n",
    "Setting this equal to $0$ and solving gives\n",
    "$$\n",
    "\\Sigma w = \\lambda w,\n",
    "$$\n",
    "the standard eigenvalue setup.\n",
    "\n",
    "So the vector $w$ that maximizes variance is an eigenvector corresponding to the largest eigenvalue of the covariance matrix of $X$.\n",
    "\n",
    "This vector is known as the first principal component. \n",
    "\n",
    "\n",
    "Note because $\\Sigma$ is an $m\\times m$ real positive symmetric matrix it has a set of $m$ eigenvalues (assuming $n > m$) with orthogonal eigenvectors. Thus the remaining principal component vectors are the eigenvectors corresponding to the eigenvalues of $\\Sigma$ in decreasing order.\n",
    "\n",
    "##### Scaling data\n",
    "\n",
    "We typically need to scale our data prior to fitting the PCA model. This is because the variance of a large scale feature is inherently larger than the variance of a small scale feature. So if we have data with vastly differing scales, we will not be recovering the \"hidden structure\" of the data, but rather showing what columns have the largest scale. A common scaling approach is to run the data through `StandardScaler` first.\n",
    "\n",
    "Note that we did not do this here, because our data were constructed to have roughly the same scale.\n",
    "\n",
    "##### In `sklearn`\n",
    "\n",
    "In `sklearn`'s `PCA` these $w$ vectors are stored in `PCA().components_`.\n",
    "\n",
    "We will now use `components_` to more explicitly describe what is going on with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrate .components here\n",
    "\n",
    "\n",
    "## define w1 and w2\n",
    "# w1 = pca.components_[0]\n",
    "# w2 = pca.components_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c472fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=.1)\n",
    "\n",
    "plt.scatter(X[13, 0], X[13, 1], marker='x', color='r', s=100)\n",
    "\n",
    "# The scalar projection is (X* dot w1 times w1, X* dot w1 times w1)\n",
    "plt.plot([X[13,0], X[13,:].dot(w1)*w1[0]], [X[13,1], X[13,:].dot(w1)*w1[1]],'r--')\n",
    "plt.plot([X[13,0], X[13,:].dot(w2)*w2[0]], [X[13,1], X[13,:].dot(w2)*w2[1]],'r--')\n",
    "\n",
    "for length, vector, name in zip(pca.explained_variance_, pca.components_, [\"$w_1$\",\"$w_2$\"]):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "    plt.text(v[0],v[1],name, fontsize=16)\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(\"$x_2$\", fontsize=14)\n",
    "\n",
    "plt.title(\"Original Data\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(14,10))\n",
    "\n",
    "plt.scatter(fit[:, 0], fit[:, 1], alpha=.1)\n",
    "plt.scatter(fit[13, 0], fit[13, 1], alpha=1, c='b', label=\"sklearn PCA\")\n",
    "\n",
    "## calculating the PCA projection by hand\n",
    "plt.scatter(X[13,:].dot(w1), X[13,:].dot(w2), c='r', marker='x', s=100, label=\"By Hand Calulation\")\n",
    "\n",
    "arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, \n",
    "                    shrinkB=0,\n",
    "                    color=\"black\")\n",
    "ax.annotate('', [3 * np.sqrt(pca.explained_variance_[0]),0], [0,0], arrowprops=arrowprops)\n",
    "plt.text(3 * np.sqrt(pca.explained_variance_[0]),0,\"$w_1$\", fontsize=16)\n",
    "ax.annotate('', [0, 3 * np.sqrt(pca.explained_variance_[1])], [0,0], arrowprops=arrowprops)\n",
    "plt.text(0,3 * np.sqrt(pca.explained_variance_[1]),\"$w_2$\", fontsize=16)\n",
    "plt.plot([X[13,:].dot(w1),X[13,:].dot(w1)], [0, X[13,:].dot(w2)],'r--')\n",
    "plt.plot([0,X[13,:].dot(w1)], [X[13,:].dot(w2), X[13,:].dot(w2)],'r--')\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.title(\"PCA Projected Data\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159ef21",
   "metadata": {},
   "source": [
    "## Explained variance\n",
    "\n",
    "For each weight vector, $w$, we call $\\text{Var}( X w)$ the explained variance due to the principal component $w$. We can think of this as the variance of $X$ explained by the prinicpal component $w$. In `sklearn` we can access this with `explained_variance_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b7aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrate explained_variance_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c0a36",
   "metadata": {},
   "source": [
    "At times it can be useful to think of this in terms of the portion of $\\text{Var}(X)$ explained by the principal direction, $w$. As we will demonstrate below, this can be helpful in determining how many components we should project down to. \n",
    "\n",
    "We can access this with `explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrate explained_variance_ratio_\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289be18",
   "metadata": {},
   "source": [
    "### Example: Labeled faces in the wild\n",
    "\n",
    "To help illustrate the utility of the explained variance we will use a new data set, the labeled faces in the wild data set, <a href=\"http://vis-www.cs.umass.edu/lfw/\">http://vis-www.cs.umass.edu/lfw/</a>. Let's demonstrate this data before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f192eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will help us get the data\n",
    "from sklearn.datasets import fetch_lfw_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b808dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "people = fetch_lfw_people(min_faces_per_person=20, resize=.7)\n",
    "\n",
    "## we'll discuss this soon\n",
    "image_shape = people.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45082c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2, 5, figsize=(15,8),\n",
    "                         subplot_kw = {'xticks':(), 'yticks':()})\n",
    "\n",
    "for target, image, ax in zip(people.target, people.images, ax.ravel()):\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(people.target_names[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8657697",
   "metadata": {},
   "source": [
    "Each observation in this data set represents a publicly sourced 87 by 65 grayscale image of a well known figure (at least well known when the data set was created). This translates to $87\\times 65 = 5{,}655$ features for each observation. \n",
    "\n",
    "### Choosing the number of  PCA components\n",
    "\n",
    "This number of features could be computationally expensive for some of the supervised learning algorithms we have learned up to this point, for example $k$-nearest neighbors. It could be useful to run PCA to reduce the number of dimensions, but how many components should we use?\n",
    "\n",
    "The answer is dependent upon your use case. For example if you are interested in using PCA to produce a visualization of the data, you likely want $2$ or $3$ components. On the other hand if you simply want to use PCA to battle colinearity for a regression model (See the PCA `Practice Problems` notebook) you will want to use as many components as columns.\n",
    "\n",
    "When the answer is not obvious from your use case, you can turn to the explained variance ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaling the data\n",
    "X = people['data']\n",
    "\n",
    "## this scales it so that that the max value\n",
    "## in a pixel is 1\n",
    "X = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the model\n",
    "## leaving it blank will have it produce the maximum number\n",
    "## of possible components\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(X)\n",
    "\n",
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7cd6d0",
   "metadata": {},
   "source": [
    "### The explained variance curve\n",
    "\n",
    "We previously mentioned the explained variance ratio as giving the portion of the original variance of $X$ explained by each principal component direction. We often look at the <i>explained variance curve</i> which plots the cumulative explained variance ratio against the number of directions you have considered. Let's do this for the PCA we just fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the explained variance curve\n",
    "## make the explained variance curve\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_)+1),\n",
    "            np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\", fontsize=18)\n",
    "plt.xlabel(\"Number of Principal Components\", fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.xlim(0,500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e3c20",
   "metadata": {},
   "source": [
    "What we typically do is look for the <i>elbow</i> in the explained variance curve. The elbow is where the amount of added variance ratio starts to rapidly decrease. We think of this as an indication that addition of another principal direst would start to provide diminishing returns.\n",
    "\n",
    "In this example it appears that the elbow appears around $100$ components.\n",
    "\n",
    "#### Alternative: setting a ratio\n",
    "\n",
    "Alternatively, we could simply set a cumulative ratio that we are happy with. This can be done quite simply with `sklearn`'s `PCA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf92e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=.95)\n",
    "\n",
    "pca.fit(X)\n",
    "\n",
    "pca.explained_variance_ratio_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00588def",
   "metadata": {},
   "source": [
    "#### Compressing the data\n",
    "\n",
    "We can think of PCA as a way to compress the data contained in $X$. \n",
    "\n",
    "A neat way to see how well the compressed data compares to the original data takes advantage of some linear algebra.\n",
    "\n",
    "Suppose that we are in $\\mathbb{R}^2$, and that vector $v$ and vector $u$ are perpendicular, then for any vector $x$,\n",
    "\n",
    "$$\n",
    "x = \\text{proj}_u (x) + \\text{proj}_v(x).\n",
    "$$\n",
    "\n",
    "Suppose that we have an observation $X^*$, then for any principal component vector $w_l$ we have that:\n",
    "\n",
    "$$\n",
    "\\text{proj}_{w_l} (X^*) = (X^* \\bullet w_l) w_l \\equiv \\tilde{X}^*_l w_l.\n",
    "$$\n",
    "\n",
    "If, as noted above, we define $\\tilde{X}^*_l$ to be the $l^{th}$ principal value for observation $*$. Then we can approximate the original $X^*$ with:\n",
    "\n",
    "$$\n",
    "X^* \\approx \\tilde{X}^*_1 w_1 + \\tilde{X}^*_2 w_2 + \\dots + \\tilde{X}^*_L w_L,\n",
    "$$\n",
    "\n",
    "where $L$ is the total number of principal components.\n",
    "\n",
    "For these images we can actually look at these approximations and judge how \"good\" they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit a PCA with 3000 comps\n",
    "pca = PCA(n_components=3000)\n",
    "pca.fit(X)\n",
    "\n",
    "## Gives the projection onto the lower dimensional PCA space\n",
    "X_tilde = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99219758",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will give you the reconstruction\n",
    "## Xtilde dot ws\n",
    "X_tilde[0,:].dot(pca.components_).reshape(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726587e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_style(\"white\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "img_selection = [0,71,201,101]\n",
    "\n",
    "fig, ax = plt.subplots(4, 7, figsize=(20,15),\n",
    "                         subplot_kw = {'xticks':(), 'yticks':()})\n",
    "\n",
    "for j in range(4):\n",
    "    ax[j,0].imshow(X[img_selection[j]].reshape(image_shape), \n",
    "                      cmap='gray')\n",
    "    ax[j,0].set_ylabel(people.target_names[people.target[img_selection[j]]], fontsize=16)\n",
    "\n",
    "ax[0,0].set_title(\"Original Image\", fontsize=16)\n",
    "\n",
    "i = 1\n",
    "for n_components in [10, 50, 100, 500, 1000, 3000]:\n",
    "    for j in range(4):\n",
    "        ax[j,i].imshow(X_tilde[img_selection[j],:n_components].dot(pca.components_[:n_components,:]).reshape(image_shape), \n",
    "                      cmap='gray')\n",
    "    ax[0,i].set_title(str(n_components)+\" Components:\\n\" +\n",
    "                      \"Cumulative\\nExplained\\nVariance\\nRatio = \" +\n",
    "                         str(np.round(100*np.sum(pca.explained_variance_ratio_[:n_components]), 2)) + \"%\", \n",
    "                      fontsize=16)\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72233c2",
   "metadata": {},
   "source": [
    "So it looks like that extra bit of explained variance makes the difference between human face and horrible ghost monster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66128768",
   "metadata": {},
   "source": [
    "## Interpreting PCA\n",
    "\n",
    "While using PCA can be incredibly useful, you will lose some of the interpretability of the original data set's features. For example, principle component 1 is a little more difficult to understand than what the original features were.\n",
    "\n",
    "Luckily, we can use the component vectors, the $w$s, to help us understand what each principal component direction is <i>capturing</i>.\n",
    "\n",
    "### Example: Basketball shot distributions\n",
    "\n",
    "To help explain how to do this we will use another new data set that tracks the shot distribution for NBA teams in the 2000-01 and the 2018-19 seasons. A basketball court can be broken into 15 unique zones like so:\n",
    "\n",
    "<img src=\"CourtZones.png\" width=\"40%;\"></img>\n",
    "\n",
    "The data set `nba_team_shots.csv` measures what fraction of every team's shots were taken in each of the 15 zones. Let's load the data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = pd.read_csv(\"../../../Data/nba_team_shots.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead576d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f471f6",
   "metadata": {},
   "source": [
    "Let's imagine a setting in which someone wanted to see if they could understand the underlying structure in this data using PCA. That is let's apply PCA to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the object\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "\n",
    "## Fit the pca\n",
    "X_scaled = scaler.fit_transform(shots[shots.columns[3:]])\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "\n",
    "## get the PCA project\n",
    "fit = pca.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.scatter(fit[:,0], fit[:,1])\n",
    "\n",
    "plt.xlabel(\"First PCA Direction\", fontsize=18)\n",
    "plt.ylabel(\"Second PCA Direction\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a9f12",
   "metadata": {},
   "source": [
    "This is nice, but what does it mean? We can use the component vectors to find out.\n",
    "\n",
    "Remember our goal is to find $w=(w_1,w_2,\\dots,w_m)^T$ with $||w|| = 1$, is such that $\\text{Var}(w_1 X_1 + w_2 X_2 + \\dots + w_m X_m) = \\text{Var}( X w)$. $w$ is our component vector, and we can note that it is the same dimension as each observation $X^{(i)}$. We can associate every entry of the component vector, $w_l$, with it the corresponding column of $X$. The larger the value of $w_l$, the more important the $l^{\\text{th}}$ colummn is to that principal direction.\n",
    "\n",
    "Let's look at our principal component vectors then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919759bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "component_vectors = pd.DataFrame(pca.components_.transpose(),\n",
    "                                    columns = ['component_1', 'component_2'],\n",
    "                                    index = shots.columns[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "component_vectors.sort_values('component_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f9da16",
   "metadata": {},
   "source": [
    "It is sometimes more informative to visualize this with a heatmap. Credit to Erd&#337;s Institute Member <a href=\"https://www.linkedin.com/in/patrick-vallely/\">Patrick Vallely</a> for the below images.\n",
    "\n",
    "<img src=\"ZonePCAHeat.png\" width=\"75%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f301ab7",
   "metadata": {},
   "source": [
    "What do these two vectors mean? For any principal component vector, positive (meaning greater than 0) vector entries correspond to more positive principal component values. For example a team with 100% of their shots from Zone 12 will have as positive a first principal component value as possible because that is the most negative row of the first principal component vector table. On the other hand, a team that shot 100% of their shots from Zone 5 will have as positive a first principal component value as possible because that is the most positive row of the first principal component vector table. \n",
    "\n",
    "In this way we can read through the principal components and see which features are being picked up in that component. We can also see how this gets reflected in the PCA plot.\n",
    "\n",
    "\n",
    "##### Blog plug\n",
    "\n",
    "If this problem interested you, check out my blog post using this data :)\n",
    "\n",
    "<a href=\"http://matthew-osborne.com/mtodata/Posts/PCA-in-NBA.html\">http://matthew-osborne.com/mtodata/Posts/PCA-in-NBA.html</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519d5ba",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "<a href = \"https://www.tandfonline.com/doi/abs/10.1080/14786440109462720\">On lines and planes of closest fit to systems of points in space</a>\n",
    "\n",
    "<a href=\"https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\">Univeristy of Waterloo Matrix Cookbook</a>\n",
    "\n",
    "<a href=\"http://www.math.kent.edu/~reichel/courses/monte.carlo/alt4.7d.pdf\">Kent State University Notes on Random Vectors and Matrices</a>\n",
    "\n",
    "<a href=\"http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/pca.pdf\">Columbia PCA notes</a>\n",
    "\n",
    "<a href=\"https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\">Central Michigan PCA notes</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c22b2",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea5d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
