{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519a99a9",
   "metadata": {},
   "source": [
    "# Problem Session 8\n",
    "## Classifying Cancer II\n",
    "\n",
    "In this notebook you continue to work with the cancer data set that can be found here, <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29</a>. \n",
    "\n",
    "The problems in this notebook will cover the content covered in some of our `Classification` notebooks as well as some of our `Dimension Reduction` notebooks. In particular we will cover content touched on in:\n",
    "- `Classification/Adjustments for Classification`,\n",
    "- `Classification/k Nearest Neighbors`,\n",
    "- `Classification/The Confusion Matrix`,\n",
    "- `Classification/Logistic Regression`,\n",
    "- `Classification/Diagnostic Curves`,\n",
    "- `Classification/Bayes Based Classifiers` and\n",
    "- `Dimension Reduction/Principal Components Analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970bd0e2",
   "metadata": {},
   "source": [
    "##### 1. Load the data.\n",
    "\n",
    "The data for this problem is stored in `sklearn`, here is the documentation page for that, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html</a>.\n",
    "\n",
    "Run this code chunk to load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ae83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd3df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads the data from sklearn \n",
    "cancer = load_breast_cancer(as_frame=True)\n",
    "\n",
    "## the 'data' entry contains the features\n",
    "X = cancer['data']\n",
    "\n",
    "## the 'target' entry contains what we would like to predict\n",
    "y = cancer['target']\n",
    "\n",
    "## Chaning the labels around\n",
    "y = -y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(),\n",
    "                                                       shuffle=True,\n",
    "                                                       random_state=214,\n",
    "                                                       stratify=y,\n",
    "                                                       test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2185f",
   "metadata": {},
   "source": [
    "##### 2. Remind yourselves\n",
    "\n",
    "Take a few minutes to review `Problem Session 7` if you need to to remind yourselves about this data set and the overall goal of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539bd69e",
   "metadata": {},
   "source": [
    "##### 3. Dimension reduction\n",
    "\n",
    "The cancer data set has $30$ features. While this does not seem like a large number of features, there may be a number of features that add <i>noise</i> to the data set with respect to separating our variable of interest, $y$.\n",
    "\n",
    "One way to denoise the data is to run it through a dimension reduction technique. \n",
    "\n",
    "Run the features of the training set through principal components analysis (PCA) and reduce $30$ features down to $2$. Make a scatter plot with the first PCA values plotted on the horizontal axis and the second PCA values plotted on the vertical, color the points by their $y$ values. Comment on what you see.\n",
    "\n",
    "<i>Hint: Remember that you have to scale the data prior to fitting the PCA.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352feb8",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import what you'll need\n",
    "from sklearn.preprocessing import \n",
    "from sklearn.pipeline import \n",
    "from sklearn.decomposition import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea671a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a pipeline for your PCA\n",
    "pipe = Pipeline()\n",
    "\n",
    "## fit the pipeline\n",
    "\n",
    "\n",
    "## Get the PCA transformed data here\n",
    "fit = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code plots the Second PCA value\n",
    "## against the First PCA value for you\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.scatter(fit[y_train==0, 0],\n",
    "               fit[y_train==0, 1],\n",
    "               c = 'b',\n",
    "               alpha = .6,\n",
    "               label='Benign')\n",
    "\n",
    "plt.scatter(fit[y_train==1, 0],\n",
    "               fit[y_train==1, 1],\n",
    "               c = 'orange',\n",
    "               marker = 'v',\n",
    "               alpha = .6,\n",
    "               label='Malignant')\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"First PCA Value\", fontsize=16)\n",
    "plt.ylabel(\"Second PCA Value\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d46ee",
   "metadata": {},
   "source": [
    "##### 4. Build a Model \n",
    "\n",
    "Fill in the cross-validation code provided below to build a $k$-nearest neighbors model using the PCA processed data you generated above. What was the average TPR, FPR and precision for such a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef30869",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import what you need here\n",
    "from sklearn.model_selection import \n",
    "from sklearn.metrics import \n",
    "from sklearn.neighbors import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the kfold for you\n",
    "kfold = StratifiedKFold(5, shuffle=True, random_state=14235)\n",
    "\n",
    "## Make zero arrays to store the metrics\n",
    "## over the CV\n",
    "knn_tprs = \n",
    "knn_fprs = \n",
    "knn_precs = \n",
    "\n",
    "\n",
    "## counter to keep track of the split\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(X_train, y_train.values):\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    ## Make the knn pipeline here\n",
    "    ## Use 10 neighbors in the KNN\n",
    "    knn_pipe = Pipeline()\n",
    "    \n",
    "    ## fit the pipeline here\n",
    "    knn_pipe\n",
    "\n",
    "    ## Get the prediction from the pipeline\n",
    "    pred = knn_pipe.predict(X_ho.values)\n",
    "\n",
    "    ## Compute the precision, tpr and fpr here\n",
    "    knn_precs[i] = \n",
    "    \n",
    "    conf_mat = \n",
    "\n",
    "    knn_tprs[i] = \n",
    "    knn_fprs[i] = \n",
    "\n",
    "    ## increasing the counter\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here are the AVG CV metrics\n",
    "print(\"Mean CV TPR =\", np.round(np.mean(knn_tprs),3))\n",
    "print()\n",
    "print(\"Mean CV FPR =\", np.round(np.mean(knn_fprs),3))\n",
    "print()\n",
    "print(\"Mean CV Prec =\", np.round(np.mean(knn_precs),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57de9b",
   "metadata": {},
   "source": [
    "##### 5. Optimizing explained variance\n",
    "\n",
    "In our PCA lecture notebooks we discussed how you could choose a number of PCA components using the explained variance ratio. However, when you use PCA as a preprocessing step for a supervised learning algorithm you can perform a cross-validation optimization instead.\n",
    "\n",
    "Fill in the missing code in the chunks below to tune the fraction of total explained variance used in the PCA step.\n",
    "\n",
    "What are the explained variance ratios with the best average CV TPR, FPR and precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Explained Variance Ratios we'll try\n",
    "fracs = np.arange(.01, 1, .01)\n",
    "\n",
    "## These will track the performance across split and \n",
    "## explained variance ratio\n",
    "pca_tprs = np.zeros((5, len(fracs)))\n",
    "pca_fprs = np.zeros((5, len(fracs)))\n",
    "pca_precs = np.zeros((5, len(fracs)))\n",
    "\n",
    "## split counter\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(X_train, y_train.values):\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    ## explained variance ratio counter\n",
    "    j = 0\n",
    "    for frac in fracs:\n",
    "        ## Build the PCA pipeline here\n",
    "        ## again use 10 for k\n",
    "        ## and frac for the PCA input\n",
    "        knn_pipe = Pipeline()\n",
    "        \n",
    "        ## Fit the pipe here\n",
    "        knn_pipe\n",
    "        \n",
    "        ## Get the predictions on the holdout set\n",
    "        pred = \n",
    "\n",
    "        ## Record the metrics\n",
    "        pca_precs[i,j] = \n",
    "        \n",
    "        \n",
    "        conf_mat = \n",
    "        \n",
    "        pca_tprs[i,j] = \n",
    "        pca_fprs[i,j] = \n",
    "        j = j + 1\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will print out the avg performance metrics\n",
    "print(\"TPR\")\n",
    "print(\"==============================\")\n",
    "print(\"The explained variance ration with the highest avg. cv TPR was\",\n",
    "          fracs[np.argmax(np.mean(pca_tprs, axis=0))])\n",
    "print(\"This produced a model with avg. cv. TPR of\",np.round(np.max(np.mean(pca_tprs, axis=0)),4))\n",
    "print()\n",
    "\n",
    "print(\"FPR\")\n",
    "print(\"==============================\")\n",
    "print(\"The explained variance ration with the lowest avg. cv FPR was\",\n",
    "          fracs[np.argmin(np.mean(pca_fprs, axis=0))])\n",
    "print(\"This produced a model with avg. cv. FPR of\",np.round(np.min(np.mean(pca_fprs, axis=0)),4))\n",
    "print()\n",
    "\n",
    "print(\"Precision\")\n",
    "print(\"==============================\")\n",
    "print(\"The explained variance ration with the highest avg. cv Precision was\",\n",
    "          fracs[np.argmax(np.mean(pca_precs, axis=0))])\n",
    "print(\"This produced a model with avg. cv. Precision of\",np.round(np.max(np.mean(pca_precs, axis=0)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb9edd",
   "metadata": {},
   "source": [
    "##### 6. Make some diagnostic curves\n",
    "\n",
    "While you found the explained variance ratio with the best TPR, FPR and precision, we are often interested in the tradeoffs between such metrics. For example, a true postive rate of $1$ could be useless if that comes with a very high false positive rate.\n",
    "\n",
    "Plot the the average CV performance for all three of these metrics against the explained variance ratio in the same figure.\n",
    "\n",
    "Then thinking in terms of what it translates to for our problem of classifying cancer, select a value for the explained variance ratio to form a final PCA $k$NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c800a",
   "metadata": {},
   "source": [
    "##### 7. Trying Bayes based classifiers\n",
    "\n",
    "Build LDA, QDA and naive Bayes' models on these data by filling in the missing code for the cross-validation below. \n",
    "\n",
    "Do these outperform your PCA-$k$NN models from above?\n",
    "\n",
    "<i>Hint: You should scale the data prior to fitting the models.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import what you need here\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Makes kfold object\n",
    "kfold = StratifiedKFold(5, shuffle=True, random_state=14235)\n",
    "\n",
    "## these keep track for you\n",
    "da_tprs = np.zeros((5,3))\n",
    "da_fprs = np.zeros((5,3))\n",
    "da_precs = np.zeros((5,3))\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(X_train, y_train.values):\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    \n",
    "    ## Build the three models\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Fit the three models\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Get the predictions for the three models\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ## Record the precisions for all 3 models\n",
    "    da_precs[i,0] = \n",
    "    da_precs[i,1] = \n",
    "    da_precs[i,2] = \n",
    "\n",
    "    \n",
    "    ## Record the confusion matrices\n",
    "    lda_conf_mat = \n",
    "    qda_conf_mat = \n",
    "    nb_conf_mat = \n",
    "\n",
    "\n",
    "    ## Record the TPRs and FPRs here\n",
    "    da_tprs[i,0] = \n",
    "    da_fprs[i,0] = \n",
    "\n",
    "    da_tprs[i,1] = \n",
    "    da_fprs[i,1] = \n",
    "    \n",
    "    da_tprs[i,2] = \n",
    "    da_fprs[i,2] = \n",
    "\n",
    "\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee06dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine the performance here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db248c97",
   "metadata": {},
   "source": [
    "##### 8. LDA for supervised dimensionality reduction\n",
    "\n",
    "While we introduced linear discriminant analysis (LDA) as a classification algorithm, it was originally proposed by Fisher as a supervised dimension reduction technique, <a href=\"https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf\">https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf</a>. In particular, the initial goal was to project the features, $X$, corresponding to a binary output, $y$, onto a single dimension which best separates the possible classes. This single dimension has come to been known as <i>Fisher's discriminant</i>.\n",
    "\n",
    "Walk through the code below to perform this supervised dimension reduction technique on these Cancer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we make a validation set for demonstration purposes\n",
    "X_tt, X_val, y_tt, y_val = train_test_split(X_train.copy(), y_train,\n",
    "                                               shuffle=True,\n",
    "                                               random_state=302,\n",
    "                                               test_size = .2,\n",
    "                                               stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a pipeline that scales the data\n",
    "## and ends with LDA\n",
    "pipe = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the pipeline like you would\n",
    "## for a classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dff5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now instead of pipe.predict\n",
    "## call pipe.transform, like a PCA object\n",
    "## use the training features as input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60123b86",
   "metadata": {},
   "source": [
    "Here we have projected this $30$-dimensional data onto a $1$-dimensional space that maximizes the separation between classes $0$ and $1$. We can visualize this by plotting a histogram split by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be59ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "## Place the Fisher discriminant for the two classes of tumor here\n",
    "plt.hist(, color='blue', label=\"Benign\")\n",
    "\n",
    "## And here\n",
    "plt.hist(, color='orange', hatch='/', alpha=.6, label=\"Malignant\")\n",
    "\n",
    "plt.xlabel(\"Fisher Discriminant\", fontsize=16)\n",
    "plt.ylabel(\"Count\", fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlim([-5,8])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc4e58",
   "metadata": {},
   "source": [
    "From this we can see there is very little overlap in the values of the Fisher discriminant for the two types of tumor.\n",
    "\n",
    "We could use the discriminant in order to make classifications, for example by setting a simple cutoff value or as input into a different classification algorithm.\n",
    "\n",
    "However, it is important to note that the LDA algorithm maximizes the separation of the two classes among observations of the training set. It is possible that such good separation would not occur using data the algorithm was not trained on.\n",
    "\n",
    "In this example we can visually inspect by plotting a histogram of the Fisher discriminant values for the validation set we created. Does the separation seem as pronounced on the validation data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The same plot as above but now on the validation set\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "## Place the Fisher discriminant for the two classes of tumor here\n",
    "plt.hist(, color='blue', label=\"Benign\")\n",
    "\n",
    "## and here\n",
    "plt.hist(, color='orange', hatch='/', alpha=.6, label=\"Malignant\")\n",
    "\n",
    "plt.xlabel(\"Fisher Discriminant\", fontsize=16)\n",
    "plt.ylabel(\"Count\", fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlim([-5,8])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4598a",
   "metadata": {},
   "source": [
    "I would say that there does appear to be slightly more overlap for the validation data than the training data, but this is still a good amount of separation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edc616",
   "metadata": {},
   "source": [
    "<i>For those interested in how the supervised dimension reduction aspect of LDA works see the `Bayes Based Classifiers` `Practice Problems` notebook.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8437e1",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24945ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
