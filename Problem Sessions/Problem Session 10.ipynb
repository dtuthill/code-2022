{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b15996b",
   "metadata": {},
   "source": [
    "# Problem Session 10\n",
    "## Classifying Cancer IV\n",
    "\n",
    "This notebook will be the final time you work with the cancer data set that can be found here, <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29</a>. \n",
    "\n",
    "\n",
    "The problems in this notebook will cover the content covered in some of our `Classification`, `Dimension Reduction` and our `Ensemble Learning` notebooks. In particular we will cover content touched on in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17231c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b3003",
   "metadata": {},
   "source": [
    "##### 1. Load, train test split\n",
    "\n",
    "First load the data then make a train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c990c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a316b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads the data from sklearn \n",
    "cancer = load_breast_cancer(as_frame=True)\n",
    "\n",
    "## the 'data' entry contains the features\n",
    "X = cancer['data']\n",
    "\n",
    "## the 'target' entry contains what we would like to predict\n",
    "y = cancer['target']\n",
    "\n",
    "## switching labels for malignant and benign\n",
    "y = -y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75505be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(),\n",
    "                                                       shuffle=True,\n",
    "                                                       random_state=354,\n",
    "                                                       test_size=.2,\n",
    "                                                       stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4980346",
   "metadata": {},
   "source": [
    "##### 2. What have we done?\n",
    "\n",
    "Below I will provide you with a quick refresher of everything we have done with this data set up to this point.\n",
    "\n",
    "##### Classifying Cancer I\n",
    "\n",
    "In Classifying Cancer I you:\n",
    "- Explored the data set,\n",
    "- Considered different performance metrics for a classifier built on these data,\n",
    "- Built some logistic regression models using a single feature,\n",
    "- Build a $k$-nearest neighbors model and\n",
    "- Interpreted your model in terms of what a $1$ or $0$ indicates for a patients probability of having a malignant tumor.\n",
    "\n",
    "##### Classifying Cancer II\n",
    "\n",
    "In Classifying Cancer II you:\n",
    "- Examined the impact of PCA on your $k$NN model,\n",
    "- Tried some Bayes' based classifiers and\n",
    "- Produced Fisher discriminants for these data.\n",
    "\n",
    "##### Classifying Cancer III\n",
    "\n",
    "In Classifying Cancer III you:\n",
    "- Combined a support vector classifier with PCA,\n",
    "- Built a decision tree model and\n",
    "- Built a random forest model.\n",
    "\n",
    "In today's notebook you will look to build a couple of ensemble models with the hope that we could hone in on a final model choice on these data. In particular, you will look for models that maximize the TPR, FPR and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1d7cd",
   "metadata": {},
   "source": [
    "##### 3. A Baseline.\n",
    "\n",
    "A common baseline for classification is just randomly guessing, i.e. a biased coin flip where the probability of heads is equal to the probability of being class $1$.\n",
    "\n",
    "Use the training set to get an estimate of the TPR, FPR and precision of such a model. \n",
    "\n",
    "<i>Note: a point estimate is fine for this problem. No need to estimate over several runs and get an average.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df75049",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58475d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd9f86",
   "metadata": {},
   "source": [
    "##### 4. AdaBoost\n",
    "\n",
    "Now try to build an adaptive boosting classifier on these data.\n",
    "\n",
    "Use cross-validation to determine the optimal value for `n_estimators` for an `AdaBoostClassifier` using a `DecisionTreeClassifier` with `max_depth=2` as its base estimator. Set the `learning_rate=1` in the `AdaBoostClassifier`.\n",
    "\n",
    "To help save you some time I have provided most of the cross-validation code for you and have commented portions where you will need to fill in to get to your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the appropriate items here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll search from 1 to 100 weak learners\n",
    "num_learners = range(1,101)\n",
    "\n",
    "## Make an array to hold your cross validation results here\n",
    "ada_tprs = np.zeros((5, len(num_learners)))\n",
    "ada_fprs = np.zeros((5, len(num_learners)))\n",
    "ada_precs = np.zeros((5, len(num_learners)))\n",
    "\n",
    "\n",
    "## fill in the code for a 5-fold cross-validation object here\n",
    "kfold = \n",
    "\n",
    "\n",
    "i=0\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    # this will help you keep track of what kfold split you are on\n",
    "    print(i)\n",
    "    \n",
    "    # this gets the training and holdout sets\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    # here we loop through the different values for n_estimators\n",
    "    for j in num_learners:\n",
    "        # make the AdaBoostClassifier object here\n",
    "        ada = \n",
    "        \n",
    "        \n",
    "        # fit that object here\n",
    "        \n",
    "        \n",
    "        # get the prediction on the holdout data\n",
    "        pred = \n",
    "        \n",
    "        # record the performance in your array here\n",
    "        conf_mat = confusion_matrix(y_ho, pred)\n",
    "        \n",
    "        ada_tprs[i,j-1] = \n",
    "        ada_fprs[i,j-1] = \n",
    "        ada_precs[i,j-1] = \n",
    "        \n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aba7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3,1, figsize=(14,10), sharex=True)\n",
    "\n",
    "## TPR\n",
    "ax[0].plot(num_learners, , '-o')\n",
    "ax[0].set_ylabel(\"True Positive Rate\", fontsize=16)\n",
    "\n",
    "## FPR\n",
    "ax[1].plot(num_learners, , '-o')\n",
    "ax[1].set_ylabel(\"False Positive Rate\", fontsize=16)\n",
    "\n",
    "## Precs\n",
    "ax[2].plot(num_learners, , '-o')\n",
    "ax[2].set_ylabel(\"Precision\", fontsize=16)\n",
    "\n",
    "ax[2].set_xlabel(\"Number of weak learners\", fontsize=16)\n",
    "ax[2].set_xticks(np.arange(0,110,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41728d25",
   "metadata": {},
   "source": [
    "<i>How many weak learners would you choose for this model?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602f2ea",
   "metadata": {},
   "source": [
    "##### 5. Comparing different models\n",
    "\n",
    "Use $5$-fold cross-validation to compare the following models:\n",
    "- A pipeline that runs the data through PCA with an explained variance ratio of `0.85` and then a $k$NN model with $k = 5$,\n",
    "- A random forest model with `n_estimators=500`, `max_samples=200` and `max_depth=5`,\n",
    "- A pipeline that runs the data through PCA with an explained variance ratio of `0.90` and then a support vector machine with the `rbf` kernel and `C=1`,\n",
    "- The AdaBoost model you selected from 5. above and\n",
    "- A voter model consisting of the four previous models.\n",
    "\n",
    "Again I will provide some shell code for you to fill in the blanks to help speed things along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d97c85",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import things here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f02684",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tprs = np.zeros((5, 5))\n",
    "cv_fprs = np.zeros((5, 5))\n",
    "cv_precs = np.zeros((5, 5))\n",
    "\n",
    "## fill in the code for a 5-fold cross-validation object here\n",
    "kfold = StratifiedKFold(5, shuffle=True, random_state=413)\n",
    "\n",
    "\n",
    "i=0\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    # this gets the training and holdout sets\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    ### Making Model Objects ###\n",
    "    ## Make the knn model\n",
    "    knn_pipe = \n",
    "    \n",
    "    \n",
    "    ## Make the random forest model\n",
    "    rf = \n",
    "    \n",
    "    \n",
    "    ## Make the SVM model\n",
    "    svc_pipe = \n",
    "    \n",
    "    \n",
    "    ## Make the adaboost model\n",
    "    ada = \n",
    "    \n",
    "    \n",
    "    ## Make the voting Model\n",
    "    vote = \n",
    "    \n",
    "    \n",
    "    ### Train Model objects ###\n",
    "    \n",
    "    \n",
    "    ### Get the predictions ###\n",
    "    knn_pred = \n",
    "    rf_pred = \n",
    "    svc_pred = \n",
    "    ada_pred = \n",
    "    vote_pred = \n",
    "    \n",
    "    ### Record the performance ###\n",
    "    cv_precs[i, 0] = precision_score(y_ho, knn_pred)\n",
    "    cv_precs[i, 1] = precision_score(y_ho, rf_pred)\n",
    "    cv_precs[i, 2] = precision_score(y_ho, svc_pred)\n",
    "    cv_precs[i, 3] = precision_score(y_ho, ada_pred)\n",
    "    cv_precs[i, 4] = precision_score(y_ho, vote_pred)\n",
    "    \n",
    "    \n",
    "    \n",
    "    knn_conf_mat = confusion_matrix(y_ho, knn_pred)\n",
    "    rf_conf_mat = confusion_matrix(y_ho, rf_pred)\n",
    "    svc_conf_mat = confusion_matrix(y_ho, svc_pred)\n",
    "    ada_conf_mat = confusion_matrix(y_ho, ada_pred)\n",
    "    vote_conf_mat = confusion_matrix(y_ho, vote_pred)\n",
    "    \n",
    "    cv_tprs[i, 0] = knn_conf_mat[1,1]/(knn_conf_mat[1,0] + knn_conf_mat[1,1])\n",
    "    cv_tprs[i, 1] = rf_conf_mat[1,1]/(rf_conf_mat[1,0] + rf_conf_mat[1,1])\n",
    "    cv_tprs[i, 2] = svc_conf_mat[1,1]/(svc_conf_mat[1,0] + svc_conf_mat[1,1])\n",
    "    cv_tprs[i, 3] = ada_conf_mat[1,1]/(ada_conf_mat[1,0] + ada_conf_mat[1,1])\n",
    "    cv_tprs[i, 4] = vote_conf_mat[1,1]/(vote_conf_mat[1,0] + vote_conf_mat[1,1])\n",
    "    \n",
    "    cv_fprs[i, 0] = knn_conf_mat[0,1]/(knn_conf_mat[0,0] + knn_conf_mat[0,1])\n",
    "    cv_fprs[i, 1] = rf_conf_mat[0,1]/(rf_conf_mat[0,0] + rf_conf_mat[0,1])\n",
    "    cv_fprs[i, 2] = svc_conf_mat[0,1]/(svc_conf_mat[0,0] + svc_conf_mat[0,1])\n",
    "    cv_fprs[i, 3] = ada_conf_mat[0,1]/(ada_conf_mat[0,0] + ada_conf_mat[0,1])\n",
    "    cv_fprs[i, 4] = vote_conf_mat[0,1]/(vote_conf_mat[0,0] + vote_conf_mat[0,1])\n",
    "\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code is completely ready to run ##\n",
    "## Examine the performance across all 5 models.\n",
    "fig,ax = plt.subplots(3,1,figsize=(10,12), sharex=True)\n",
    "\n",
    "## TPR\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        ax[0].scatter(np.ones(5)*i+np.random.uniform(low=-.04, high=.04,size=5), cv_tprs[:,i], marker='x', c='k', s=60, label=\"CV Split\")\n",
    "    else:\n",
    "        ax[0].scatter(np.ones(5)*i+np.random.uniform(low=-.04, high=.04,size=5), cv_tprs[:,i], marker='x', c='k', s=60)\n",
    "    \n",
    "ax[0].scatter(range(5), np.mean(cv_tprs, axis=0), c='white', edgecolor='red', s=150, alpha=.7, label=\"CV Avg.\")\n",
    "\n",
    "ax[0].set_ylabel(\"True Positive Rate\", fontsize=16)\n",
    "\n",
    "\n",
    "## FPR\n",
    "for i in range(5):\n",
    "    ax[1].scatter(np.ones(5)*i+np.random.uniform(low=-.04, high=.04,size=5), cv_fprs[:,i], marker='x', c='k', s=60)\n",
    "    \n",
    "ax[1].scatter(range(5), np.mean(cv_fprs, axis=0), c='white', edgecolor='red', s=150, alpha=.7)\n",
    "\n",
    "ax[1].set_ylabel(\"False Positive Rate\", fontsize=16)\n",
    "\n",
    "## Precision\n",
    "for i in range(5):\n",
    "    ax[2].scatter(np.ones(5)*i+np.random.uniform(low=-.04, high=.04,size=5), cv_precs[:,i], marker='x', c='k', s=60)\n",
    "    \n",
    "ax[2].scatter(range(5), np.mean(cv_precs, axis=0), c='white', edgecolor='red', s=150, alpha=.7)\n",
    "\n",
    "ax[2].set_ylabel(\"Precision\", fontsize=16)\n",
    "\n",
    "\n",
    "ax[2].set_xticks([0,1,2,3,4])\n",
    "ax[2].set_xticklabels([\"$k$NN\", \"Random\\nForest\", \"SVC\", \"AdaBoost\", \"Voter\"], fontsize=14)\n",
    "\n",
    "ax[2].set_xlabel(\"Model\", fontsize=16)\n",
    "\n",
    "ax[0].legend(fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc89fe",
   "metadata": {},
   "source": [
    "<i>Which model do you choose? Why?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962663bd",
   "metadata": {},
   "source": [
    "#### Write here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f94428",
   "metadata": {},
   "source": [
    "##### 6. A return to interpretation\n",
    "\n",
    "Recall from Classifying Cancer I\n",
    "\n",
    "Common questions for diagnostic models concern estimating the probability that an individual does or does not have a disease if the model says (or does not say) they have one. We can estimate such a statistic using Bayes' rule.\n",
    "\n",
    "$$\n",
    "P\\left(\\text{Has Cancer} | \\text{Classified } 1\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{P\\left(\\text{Classified } 1 | \\text{Has Cancer} \\right) P\\left( \\text{Has Cancer}  \\right)}{P\\left(\\text{Classified } 1 | \\text{Has Cancer} \\right) P\\left( \\text{Has Cancer}  \\right) + P\\left(\\text{Classified } 1 | \\text{Does Not Have Cancer} \\right) P\\left( \\text{Does Not Have Cancer}  \\right)},\n",
    "$$\n",
    "\n",
    "similarly\n",
    "\n",
    "$$\n",
    "P\\left(\\text{Has Cancer} | \\text{Classified } 0\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{P\\left(\\text{Classified } 0 | \\text{Has Cancer} \\right) P\\left( \\text{Has Cancer}  \\right)}{P\\left(\\text{Classified } 0 | \\text{Has Cancer} \\right) P\\left( \\text{Has Cancer}  \\right) + P\\left(\\text{Classified } 0 | \\text{Does Not Have Cancer} \\right) P\\left( \\text{Does Not Have Cancer}  \\right)},\n",
    "$$\n",
    "\n",
    "We can estimate $P\\left(\\text{Classified } 1 | \\text{Has Cancer} \\right)$ with the true positive rate and we can estimate $P(\\text{Has Cancer})$ or $P(\\text{Does Not Have Cancer})$ using the rates from the data.\n",
    "\n",
    "Estimate the true positive and true negative rates for your classifier using cross-validation. Then estimate $P\\left(\\text{Has Cancer} | \\text{Classified } 1\\right)$ and $P\\left(\\text{Has Cancer} | \\text{Classified } 0\\right)$ for your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905dab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tprs = np.zeros(5)\n",
    "model_fprs = np.zeros(5)\n",
    "model_tnrs = np.zeros(5)\n",
    "model_fnrs = np.zeros(5)\n",
    "\n",
    "has_cancer = np.zeros(5)\n",
    "no_cancer = np.zeros(5)\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    # this gets the training and holdout sets\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    \n",
    "    ## Make your model\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Record the estimate for the malignant prevalence\n",
    "    has_cancer[i] = \n",
    "    no_cancer[i] = \n",
    "    \n",
    "    ## Train your model\n",
    "    \n",
    "    \n",
    "    ## Get the prediction for your model\n",
    "    pred = \n",
    "    \n",
    "    conf_mat = confusion_matrix(y_ho, pred)\n",
    "    \n",
    "    model_tprs[i] = conf_mat[1,1]/(conf_mat[1,0] + conf_mat[1,1])\n",
    "    model_fprs[i] = conf_mat[0,1]/(conf_mat[0,0] + conf_mat[0,1])\n",
    "    model_tnrs[i] = conf_mat[0,0]/(conf_mat[0,0] + conf_mat[0,1])\n",
    "    model_fnrs[i] = conf_mat[1,0]/(conf_mat[1,0] + conf_mat[1,1])\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a18760",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculate the probabilities from above here\n",
    "p_has_cancer_given_1 = \n",
    "p_has_cancer_given_0 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ac722",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"If our classifier says a patient has cancer, we estimate\",\n",
    "      \"a\", np.round(np.mean(p_has_cancer_given_1),5),\n",
    "      \"probability that they actually have cancer.\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"If our classifier says a patient does not have cancer, we estimate\",\n",
    "      \"a\", np.round(np.mean(p_has_cancer_given_0),5),\n",
    "      \"probability that they actually do have cancer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b70a4",
   "metadata": {},
   "source": [
    "##### 7. Model more\n",
    "\n",
    "Feel free to make more models down below if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478bc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471c024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aab879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d63d22a",
   "metadata": {},
   "source": [
    "##### 8. Looking at the test set.\n",
    "\n",
    "Once you have a final model that you are pleased with, retrain the model using the entire training set. Then check the TPR, FPR and precision on the test set. How does this compare with what you expected from the cross-validation above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set TPR =\", )\n",
    "print(\"Test set FPR =\", )\n",
    "print(\"Test set precision = \", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a68fa",
   "metadata": {},
   "source": [
    "##### Write any notes you would like to here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8796a",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d75ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
