{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb09f4b",
   "metadata": {},
   "source": [
    "# Fall Problem Session 6\n",
    "## Forecasting The Bachelorette and Pumpkin Spice II\n",
    "\n",
    "In the second of two time series based problem sessions you build upon your work in `Fall Problem Session 5`. In particular you will look to build the best forecast you can for the Bachelorette IMDB ratings. Afterwards you will be introduced to seasonal ARIMA models with the pumpkin spice Google trends data.\n",
    "\n",
    "The problems in this notebook will cover the content covered in our `Time Series Forecasting` lectures including:\n",
    "- `Averaging and Smoothing`,\n",
    "- `Stationarity and Autocorrelation` and\n",
    "- `ARIMA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "from datetime import datetime\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fdf67",
   "metadata": {},
   "source": [
    "#### 1. The Bachelorette\n",
    "\n",
    "##### a.\n",
    "\n",
    "- Reload the Bachelorette IMDB data stored in `bachelorette.csv` in the `Data` folder. \n",
    "- Look at the first five rows.\n",
    "- Then make the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba536208",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = pd.read_csv(\"../Data/the_bachelorette.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951691e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_train = tv.iloc[:-3].copy()\n",
    "tv_test = tv.drop(tv_train.index).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ce001",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "Here is a refresher on the columns of this data.\n",
    "\n",
    "- `episode_number` is the number of the episode with respect to the entire series run,\n",
    "- `title` is the title of the episode,\n",
    "- `season` is the number of the season in which the episode aired,\n",
    "- `season_episode_number` is the number of the episode with respect to the season in which it aired,\n",
    "- `imdb_rating` is the average rating of the episode among IMDB's users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdff54c",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "The first model you will fit is a moving average model. In this problem you will be tuning the moving average window size, $q$, to find the value that minimizes the average cross-validation root mean squared error (RMSE).\n",
    "\n",
    "Fill in the missing chunks of code to perform hyperparameter tuning for $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e41fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import what you need for CV\n",
    "\n",
    "\n",
    "## import mse\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e830daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = \n",
    "\n",
    "start = 2\n",
    "end = 31\n",
    "ma_rmses = np.zeros((10, len(range(start, end))))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(tv_train):\n",
    "    tv_tt = tv_train.loc[train_index]\n",
    "    tv_ho = tv_train.loc[test_index]\n",
    "    \n",
    "    j = 0\n",
    "    for q in range(start, end):\n",
    "        pred = \n",
    "        \n",
    "        ma_rmses[i,j] = \n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfefec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will plot the average CV RMSE\n",
    "## against the window size\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.scatter(range(start,end), np.mean(ma_rmses, axis=0))\n",
    "\n",
    "plt.xlabel(\"Window Size\", fontsize=16)\n",
    "plt.ylabel(\"Average CV RMSE\", fontsize=16)\n",
    "\n",
    "plt.xticks(range(start, end, 3), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The window size that minimized the avg. cv rmse\",\n",
    "      \"was q =\", \n",
    "      range(start,end)[np.argmin(np.mean(ma_rmses, axis=0))],\n",
    "      \"\\b.\",\n",
    "      \"It had a mean cv rmse of\", \n",
    "      np.round(np.min(np.mean(ma_rmses, axis=0)), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100179e4",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "The second model you will try is an exponential smoothing model.\n",
    "\n",
    "Because these data exhibit a trend but not seasonality we will fit a double exponential smoothing model. For this we will want to find the best $\\alpha$ (The smoothing on the time series) and $\\beta$ (the smoothing on the trend component).\n",
    "\n",
    "Fill in the missing code chunks below to perform a grid search for the values of $\\alpha$ and $\\beta$ that minimize the average CV RMSE. (Note that a grid search is what we call it when you perform hyperparameter tuning with a grid of possible hyperparameter values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import what you need to make an exponential smoothing model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0, 0.2, .01)\n",
    "betas = np.arange(0, 0.2, .01)\n",
    "\n",
    "exp_rmses = np.zeros((10, len(alphas), len(betas)))\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(tv_train):\n",
    "    tv_tt = tv_train.loc[train_index]\n",
    "    tv_ho = tv_train.loc[test_index]\n",
    "    \n",
    "    j = 0\n",
    "    for alpha in alphas:\n",
    "        k = 0\n",
    "        for beta in betas:\n",
    "            print(\"alpha =\", alpha,\n",
    "                     \"beta =\", beta)\n",
    "\n",
    "            exp_smooth = \n",
    "\n",
    "            exp_rmses[i,j,k] = \n",
    "            \n",
    "            \n",
    "            k = k + 1\n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This gives us the indices of the smallest\n",
    "## avg cv rmse\n",
    "exp_ind = np.unravel_index(np.argmin(np.mean(exp_rmses, axis=0), axis=None), \n",
    "                           np.mean(exp_rmses, axis=0).shape)\n",
    "np.unravel_index(np.argmin(np.mean(exp_rmses, axis=0), axis=None), \n",
    "                 np.mean(exp_rmses, axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fb07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The alpha and beta values that give a double exponential\",\n",
    "         \"smoothing model with lowest avg cv rmse are\",\n",
    "         \"alpha = \", np.arange(0, 0.2, .01)[exp_ind[0]],\n",
    "         \"and beta = \", np.arange(0, 0.2, .01)[exp_ind[1]])\n",
    "\n",
    "print(\"This model had an avg cv rmse of\",\n",
    "         np.round(np.mean(exp_rmses, axis=0)[exp_ind],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3caaa",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "The final model you will try is an ARIMA model. \n",
    "\n",
    "First let's check the stationarity assumption for this time series. Make an autocorrelation plot of the training data. If you find that the ACF plot indicates that the time series is non-stationary, plot the ACF of the time series' first differences. Do these appear to be stationary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c90656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d961f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "\n",
    "## fill in the missing code to make the ACF for the original data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title('The Bachelorette IMDB rating ACF', fontsize=18)\n",
    "plt.ylabel(\"Autocorrelation\", fontsize=16)\n",
    "plt.xlabel(\"Lag\", fontsize=16)\n",
    "\n",
    "plt.ylim(-1.1,1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93aed76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077aa944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fabf1986",
   "metadata": {},
   "source": [
    "##### f.\n",
    "\n",
    "From what we saw above we should set our $d$ value in the ARIMA model to $1$. This means we still have to perform hyperparameter tuning to find the values of $p$ and $q$ that give us the lowest mean CV RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import SARIMAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_rmses = np.zeros((10, 3, 3))\n",
    "\n",
    "ps = range(1,4)\n",
    "qs = range(1,4)\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(tv_train):\n",
    "    tv_tt = tv_train.loc[train_index]\n",
    "    tv_ho = tv_train.loc[test_index]\n",
    "    \n",
    "    j = 0\n",
    "    for p in :\n",
    "        k = 0\n",
    "        for q in :\n",
    "            arima = \n",
    "            \n",
    "            \n",
    "            arima_rmses[i,j,k] = \n",
    "            \n",
    "            k = k +1\n",
    "        j = j + 1\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca355ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_ind = np.unravel_index(np.argmin(np.mean(arima_rmses, axis=0), axis=None), \n",
    "                             np.mean(arima_rmses, axis=0).shape)\n",
    "np.unravel_index(np.argmin(np.mean(arima_rmses, axis=0), axis=None), \n",
    "                 np.mean(arima_rmses, axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce5d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The p and q values that give an ARIMA model\",\n",
    "         \"with lowest avg cv mse are\",\n",
    "         \"p = \", range(1,4)[arima_ind[0]],\n",
    "         \"and q = \", range(1,4)[arima_ind[1]])\n",
    "\n",
    "print(\"This model had an avg cv mse of\",\n",
    "         np.round(np.mean(arima_rmses, axis=0)[arima_ind],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d14b7",
   "metadata": {},
   "source": [
    "##### g.\n",
    "\n",
    "Compare the best RMSE you attained in this notebook to the best RMSE for the baseline models in the completed version of `Fall Problem Session 5`.\n",
    "\n",
    "Plot the best forecast with the training and test data. What is the RMSE of the forecast on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1b292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46bdc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episode Number\", fontsize=16)\n",
    "plt.ylabel(\"IMDB Rating\", fontsize=16)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08da11",
   "metadata": {},
   "source": [
    "#### 2. Pumpkin spice seasonal ARIMA\n",
    "\n",
    "In this problem you will be introduces to seasonal ARIMA models with the pumpkin spice Google trend data. This will be a surface level introduction, for a more in depth look check out the time series practice problems `jupyter notebook`.\n",
    "\n",
    "##### a.\n",
    "\n",
    "Load the data stored in `pumpkin_spice.csv` in the `Data` folder then look at the first five rows. Then make a train test split setting aside all observations after December 31, 2021 aside as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a480b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpkin = pd.read_csv(\"../Data/pumpkin_spice.csv\",\n",
    "                         parse_dates = [\"Month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66abef58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb445ff2",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "In lecture we talked about first differencing non-stationary time series exhibiting a trend to create a, seemingly, stationary time series.\n",
    "\n",
    "This can also be done for seasonal data. Suppose that we suspect a time series, $\\left\\lbrace y_t \\right\\rbrace$ exhibits seasonality where a season lasts $m$ time steps. Then the first seasonal differenced time series is:\n",
    "\n",
    "$$\n",
    "\\nabla_s y_t = y_t - y_{t-m}.\n",
    "$$\n",
    "\n",
    "Plot the autocorrelation of the training set, then perform first seasonal differencing on these data and plot the autocorrelation of the first seasonal differenced series.\n",
    "\n",
    "Does the differenced series appear less likely to violate stationarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b56a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c780ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(12,6))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.ylim([-1.1,1.1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7560f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b10c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9958b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5776d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eafdb44d",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "While traditional $\\text{ARIMA}$ models do not work well for seasonal data, there are seasonal ARIMA ($\\text{SARIMA}$) models as well. Recall for an $\\text{ARIMA}$ model you needed parameters $p$, $d$ and $q$. For a $\\text{SARIMA}$ model you need parameters $P$, $D$, $Q$ and $m$ here:\n",
    "\n",
    "- $P$ is the order of the seasonal autoregressive portion of the model,\n",
    "- $Q$ is the order of the seasonal moving average portion of the model,\n",
    "- $D$ is the order of the seasonal differencing and\n",
    "- $m$ is the number of time steps that take place in a single period.\n",
    "\n",
    "You should have an idea of a value for $D$ from <i>b.</i> and we know $m=12$. In this problem you will fit a $\\text{SARIMA}$ model on the pumpkin spice data using `statsmodels` `SARIMAX`. Choose whatever values you would like for $p$, $P$, $q$ and $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f767932",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \n",
    "q = \n",
    "P = \n",
    "D = \n",
    "Q = \n",
    "\n",
    "\n",
    "sarima = SARIMAX(p_train.interest_level.values,\n",
    "                    order = (p,0,q), \n",
    "                    seasonal_order = (P,D,Q,12)).fit(maxiter=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.plot(\n",
    "            'b-o',\n",
    "            label='Training Data')\n",
    "\n",
    "plt.plot(\n",
    "            'r',\n",
    "            label='Fitted Values')\n",
    "\n",
    "plt.scatter(\n",
    "               c='r',\n",
    "               marker='x',\n",
    "               s=100,\n",
    "               label=\"Forecast\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=14)\n",
    "plt.ylabel(\"Interest Level\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d4590",
   "metadata": {},
   "source": [
    "##### d. \n",
    "\n",
    "Get the average cross-validation MSE for the SARIMA model you fit above. Use 5-fold cross-validation with a test set size of 12.\n",
    "\n",
    "\n",
    "How does it compare to the baseline models from `Fall Problem Session 5`? <i>Feel free to use the answer from the completed version `Fall Problem Session 5` if you did not complete it</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e49296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26114e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = np.zeros(5)\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(p_train):\n",
    "    p_tt = p_train.iloc[train_index]\n",
    "    p_ho = p_train.iloc[test_index]\n",
    "    \n",
    "    sarima = \n",
    "    \n",
    "    rmses[i] = \n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7622f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be4194",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47286d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6ebeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "badd9549",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8860f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
