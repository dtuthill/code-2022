{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb82e498",
   "metadata": {},
   "source": [
    "# Fall Problem Session 8\n",
    "## Classifying Pumpkin Seeds II\n",
    "\n",
    "In this notebook you continue to work with the pumpkin seed data from <a href=\"https://link.springer.com/article/10.1007/s10722-021-01226-0\">The use of machine learning methods in classification of pumpkin seeds (Cucurbita pepo L.)</a> by Koklu, Sarigil and Ozbek (2021).\n",
    "\n",
    "The problems in this notebook will cover the content covered in some of our `Classification` notebooks as well as some of our `Dimension Reduction` notebooks. In particular we will cover content touched on in:\n",
    "- `Classification/Adjustments for Classification`,\n",
    "- `Classification/k Nearest Neighbors`,\n",
    "- `Classification/The Confusion Matrix`,\n",
    "- `Classification/Logistic Regression`,\n",
    "- `Classification/Diagnostic Curves`,\n",
    "- `Classification/Bayes Based Classifiers` and\n",
    "- `Dimension Reduction/Principal Components Analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fde3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing all the packages you may use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96b5c96",
   "metadata": {},
   "source": [
    "#### 1. Load then prepare the data\n",
    "\n",
    "\n",
    "- Load the data stored in `Pumpkin_Seeds_Dataset.xlsx` in the `Data` folder,\n",
    "- Create a column `y` where `y=1` if `Class=Ürgüp Sivrisi` and `y=0` if `Class=Çerçevelik` and\n",
    "- Make a train test split setting $10\\%$ of the data aside as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac85df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the data using read_excel\n",
    "seeds = pd.read_excel(\"../Data/Pumpkin_Seeds_Dataset.xlsx\")\n",
    "\n",
    "## add in the y column here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82571fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5feab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the train test split here\n",
    "seeds_train, seeds_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed064102",
   "metadata": {},
   "source": [
    "#### 2. Refresh your memory\n",
    "\n",
    "If you need to refresh your memory on these data and the problem, you may want to look at a small subset of the data, look back on `Fall Problem Session 7` and/or browse Figure 5 and Table 1 of this paper, <a href=\"pumpkin_seed_paper.pdf\">pumpkin_seed_paper.pdf</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f9ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make any notes or code you'd like to in these two chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8801cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1812954f",
   "metadata": {},
   "source": [
    "#### 3. Principal components analysis (PCA)\n",
    "\n",
    "One way you may use PCA is as a data preprocessing step for supervised learning tasks. In this problem you will try it as a preprocessing step for the pumpkin seed data and see if this preprocessing step helps your model outperform the models from `Fall Problem Session 7`.\n",
    "\n",
    "##### a. \n",
    "\n",
    "Run the training data through PCA with two components and then plot the resulting principal values. Color each point by its class.\n",
    "\n",
    "<i>Hint: Remember to scale the data before running it through PCA</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import PCA\n",
    "from sklearn.decomposition import \n",
    "\n",
    "## import anything else you might need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5508c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make and fit the PCA here\n",
    "\n",
    "\n",
    "\n",
    "## get the PCA transformed data here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in the missing code for the scatter plot here\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "## plot one seed type with this\n",
    "plt.scatter(,\n",
    "            ,\n",
    "            color=,\n",
    "            label=)\n",
    "\n",
    "## plot the other seed type here\n",
    "plt.scatter(, \n",
    "            ,\n",
    "            color=,\n",
    "            marker='v',\n",
    "            label=)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"First PCA Value\", fontsize=16)\n",
    "plt.ylabel(\"Second PCA Value\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9544b1b",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "How does the PCA with only two componenets appear to separate the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da135b0",
   "metadata": {},
   "source": [
    "##### Write your thoughts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f34cd",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "Run 10-fold cross-validation below to find the optimal value of $k$ for a $k$ nearest neighbors model fit on the first and second PCA values. What is the optimal $k$ and the associated average cross-validation accuracy? How does this compare to the accuracies from `Fall Problem Session 7`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing what you'll use in this problem \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the kfold object here\n",
    "n_splits=10\n",
    "kfold = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your potential range of k values\n",
    "ks = range(1, 51)\n",
    "\n",
    "## this will hold the accuracy scores\n",
    "pca_2_accs = np.zeros((n_splits, len(ks)))\n",
    "\n",
    "i = 0\n",
    "## fill in the missing pieces below\n",
    "## looping through the training and holdout splits of CV\n",
    "for train_index, test_index in :\n",
    "    seeds_tt = seeds_train.iloc[train_index]\n",
    "    seeds_ho = seeds_train.iloc[test_index]\n",
    "    \n",
    "    j = 0\n",
    "    ## looping throught he different numbers of neigbors\n",
    "    for k in ks:\n",
    "        ## make your PCA knn pipeline here\n",
    "        pipe = \n",
    "        \n",
    "        ## fit the pipeline here\n",
    "        pipe\n",
    "        \n",
    "        ## get your prediction on the holdout set\n",
    "        pred = \n",
    "        \n",
    "        \n",
    "        ## recording the accuracy score on the holdout set\n",
    "        pca_2_accs[i,j] = accuracy_score(seeds_ho.y.values, pred)\n",
    "        \n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will plot the average CV accuracy for you\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "\n",
    "plt.plot(ks, \n",
    "         np.mean(pca_2_accs, axis=0),\n",
    "         '-o')\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlabel(\"$k$\", fontsize=16)\n",
    "plt.ylabel(\"Avg. CV Accuracy\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c2b51",
   "metadata": {},
   "source": [
    "##### Write any notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f636ad5",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "We discussed examining the explained variance ratio as a way to determine how many components we may want to use in our PCA. We can also think of the explained variance as another hyperparameter we can tune through cross-validation.\n",
    "\n",
    "Fill in the missing code below to find the optimal explained variance ratio and $k$ pairing for this problem. What is the best average cross-validation accuracy? How many components did you end up needing to achieve the highest average cross-validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## possible k values\n",
    "ks = range(1, 51)\n",
    "\n",
    "## possible explained variance values\n",
    "explained_variances = np.arange(.09, 1, .1)\n",
    "\n",
    "## this holds the cv accuracies\n",
    "pca_accs = np.zeros((n_splits, len(ks), len(explained_variances)))\n",
    "\n",
    "i = 0\n",
    "## looping through the train and holdout splits\n",
    "for train_index, test_index in kfold.split(seeds_train, seeds_train.y):\n",
    "    ## the training data\n",
    "    seeds_tt = seeds_train.iloc[train_index]\n",
    "    ## the holdout data\n",
    "    seeds_ho = seeds_train.iloc[test_index]\n",
    "    \n",
    "    j = 0\n",
    "    ## loop through the neighbor possibilities for knn\n",
    "    ## note the use of neighbors instead of k\n",
    "    for neighbors in ks:\n",
    "        k = 0\n",
    "        \n",
    "        ## loop through the different explained variance values\n",
    "        for var in explained_variances:\n",
    "            ## make the pipeline\n",
    "            pipe = \n",
    "\n",
    "            ## fit the pipeline\n",
    "            pipe.\n",
    "\n",
    "            ## make the prediction on the holdout set\n",
    "            pred = pipe.\n",
    "\n",
    "            ## record the accuracy\n",
    "            pca_accs[i,j,k] = accuracy_score(seeds_ho.y.values, pred)\n",
    "            k = k + 1\n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f71714",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code will tell you the optimal variance-k pairing\n",
    "max_index = np.unravel_index(np.argmax(np.mean(pca_accs, axis=0), axis=None), \n",
    "                                       np.mean(pca_accs, axis=0).shape)\n",
    "\n",
    "\n",
    "print(\"The pair with the highest AVG CV Accuracy was\",\n",
    "         \"k =\", ks[max_index[0]],\n",
    "         \"and explained variance =\", np.round(explained_variances[max_index[1]],2))\n",
    "print(\"The highest AVG CV Accuracy was\", np.max(np.mean(pca_accs, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the number of components here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae7853",
   "metadata": {},
   "source": [
    "#### 4. Trying Bayes based classifiers\n",
    "\n",
    "Build LDA, QDA and naive Bayes' models on these data by filling in the missing code for the cross-validation below. \n",
    "\n",
    "Do these outperform your PCA-$k$NN model from above?\n",
    "\n",
    "<i>Note: it is good practice to scale your data prior to fitting LDA, QDA or Naive Bayes</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25825cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import what you need here\n",
    "\n",
    "## import LDA and QDA\n",
    "from sklearn.discriminant_analysis\n",
    "## import GaussianNB\n",
    "from sklearn.naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcf3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will hold the accuracies\n",
    "bayes_accs = np.zeros((n_splits, 3))\n",
    "\n",
    "\n",
    "i = 0\n",
    "## looping through the train and holdout splits of CV\n",
    "for train_index, test_index in kfold.split(seeds_train, seeds_train.y):\n",
    "    seeds_tt = seeds_train.iloc[train_index]\n",
    "    seeds_ho = seeds_train.iloc[test_index]\n",
    "    \n",
    "    #### Don't forget to scale the data first ####\n",
    "    ## Linear Discriminant Analysis\n",
    "    lda = \n",
    "    \n",
    "    lda.fit(seeds_tt[features].values,\n",
    "               seeds_tt.y.values)\n",
    "    lda_pred = lda.predict(seeds_ho[features].values)\n",
    "    \n",
    "    bayes_accs[i, 0] = accuracy_score(seeds_ho.y.values,\n",
    "                                         lda_pred)\n",
    "    \n",
    "    ## Quadratic Discriminant Analysis\n",
    "    qda = \n",
    "    \n",
    "    \n",
    "    qda.fit(seeds_tt[features].values,\n",
    "               seeds_tt.y.values)\n",
    "    \n",
    "    qda_pred = qda.predict(seeds_ho[features].values)\n",
    "    \n",
    "    bayes_accs[i, 1] = accuracy_score(seeds_ho.y.values,\n",
    "                                         qda_pred)\n",
    "    \n",
    "    \n",
    "    ## Gaussian Naive Bayes\n",
    "    nb = \n",
    "    \n",
    "    nb.fit(seeds_tt[features].values,\n",
    "              seeds_tt.y.values)\n",
    "    \n",
    "    nb_pred = nb.predict(seeds_ho[features].values)\n",
    "    \n",
    "    bayes_accs[i, 2] = accuracy_score(seeds_ho.y.values,\n",
    "                                         nb_pred)\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the average cv accuracies for these models\n",
    "np.mean(bayes_accs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ed6e5",
   "metadata": {},
   "source": [
    "##### Make any notes you'd like here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3d31c",
   "metadata": {},
   "source": [
    "#### 5. LDA for supervised dimensionality reduction\n",
    "\n",
    "While we introduced linear discriminant analysis (LDA) as a classification algorithm, it was originally proposed by Fisher as a supervised dimension reduction technique, <a href=\"https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf\">https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf</a>. In particular, the initial goal was to project the features, $X$, corresponding to a binary output, $y$, onto a single dimension which best separates the possible classes. This single dimension has come to been known as <i>Fisher's discriminant</i>.\n",
    "\n",
    "Walk through the code below to perform this supervised dimension reduction technique on these data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59195dfd",
   "metadata": {},
   "source": [
    "##### a.\n",
    "\n",
    "First make a validation set from the training set for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we make a validation set for demonstration purposes\n",
    "seed_tt, seeds_val = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405fe4f",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "Now make a pipeline that first scales the data and ends with linear discriminant analysis. Then fit the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c13d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "968eeac5",
   "metadata": {},
   "source": [
    "##### c. \n",
    "\n",
    "Now calculate the Fisher discriminant by using `transform` with the pipeline you fit in <i>b.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339c495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de305b1",
   "metadata": {},
   "source": [
    "##### d. \n",
    "\n",
    "To visualize how LDA separated the two classes while projecting the 12 dimensional data onto a one dimensional subspace you can plot a histogram of the Fisher discriminant colored by the pumpkin seed class of the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "## plot the histogram of discriminant values (what you got from part c)\n",
    "## for one seed type here, make sure to include a label\n",
    "plt.hist()\n",
    "\n",
    "## plot the histogram of discriminant values\n",
    "## for the other seed type here, make sure to include a label\n",
    "plt.hist()\n",
    "\n",
    "plt.xlabel(\"Fisher Discriminant\", fontsize=16)\n",
    "plt.ylabel(\"Count\", fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dbac94",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "While there is some separation between the two classes, it is not perfect, this should be expected based on the exploratory data analysis you did in `Fall Problem Session 7`.\n",
    "\n",
    "We could use this discriminant in order to make classifications, for example by setting a simple cutoff value or as input into a different classification algorithm.\n",
    "\n",
    "However, it is important to note that the LDA algorithm maximizes the separation of the two classes among observations of the training set. It is possible that separation would not be as good for data the algorithm was not trained on.\n",
    "\n",
    "In this example we can visually inspect by plotting a histogram of the Fisher discriminant values for the validation set we created. Does the separation seem as pronounced on the validation data? Investigate this by examining the histograms for the Fisher discriminants of the validation set.\n",
    "\n",
    "<i>Note: remember that when getting the discriminants on the validation set you should not refit the LDA model, just use `transform`.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3350778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc87656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1636ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5efd76a",
   "metadata": {},
   "source": [
    "##### Make any notes you'd like here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9861ac",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2022.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13cbc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
